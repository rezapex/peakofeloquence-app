{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:pip:_vendor:pygments:lexer.py","body":"\"\"\"\n    pygments.lexer\n    ~~~~~~~~~~~~~~\n\n    Base lexer classes.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nimport re\nimport sys\nimport time\n\nfrom pip._vendor.pygments.filter import apply_filters, Filter\nfrom pip._vendor.pygments.filters import get_filter_by_name\nfrom pip._vendor.pygments.token import Error, Text, Other, Whitespace, _TokenType\nfrom pip._vendor.pygments.util import get_bool_opt, get_int_opt, get_list_opt, \\\n    make_analysator, Future, guess_decode\nfrom pip._vendor.pygments.regexopt import regex_opt\n\n__all__ = ['Lexer', 'RegexLexer', 'ExtendedRegexLexer', 'DelegatingLexer',\n           'LexerContext', 'include', 'inherit', 'bygroups', 'using', 'this',\n           'default', 'words', 'line_re']\n\nline_re = re.compile('.*?\\n')\n\n_encoding_map = [(b'\\xef\\xbb\\xbf', 'utf-8'),\n                 (b'\\xff\\xfe\\0\\0', 'utf-32'),\n                 (b'\\0\\0\\xfe\\xff', 'utf-32be'),\n                 (b'\\xff\\xfe', 'utf-16'),\n                 (b'\\xfe\\xff', 'utf-16be')]\n\n_default_analyse = staticmethod(lambda x: 0.0)\n\n\nclass LexerMeta(type):\n    \"\"\"\n    This metaclass automagically converts ``analyse_text`` methods into\n    static methods which always return float values.\n    \"\"\"\n\n    def __new__(mcs, name, bases, d):\n        if 'analyse_text' in d:\n            d['analyse_text'] = make_analysator(d['analyse_text'])\n        return type.__new__(mcs, name, bases, d)\n\n\nclass Lexer(metaclass=LexerMeta):\n    \"\"\"\n    Lexer for a specific language.\n\n    See also :doc:`lexerdevelopment`, a high-level guide to writing\n    lexers.\n\n    Lexer classes have attributes used for choosing the most appropriate\n    lexer based on various criteria.\n\n    .. autoattribute:: name\n       :no-value:\n    .. autoattribute:: aliases\n       :no-value:\n    .. autoattribute:: filenames\n       :no-value:\n    .. autoattribute:: alias_filenames\n    .. autoattribute:: mimetypes\n       :no-value:\n    .. autoattribute:: priority\n\n    Lexers included in Pygments should have an additional attribute:\n\n    .. autoattribute:: url\n       :no-value:\n\n    You can pass options to the constructor. The basic options recognized\n    by all lexers and processed by the base `Lexer` class are:\n\n    ``stripnl``\n        Strip leading and trailing newlines from the input (default: True).\n    ``stripall``\n        Strip all leading and trailing whitespace from the input\n        (default: False).\n    ``ensurenl``\n        Make sure that the input ends with a newline (default: True).  This\n        is required for some lexers that consume input linewise.\n\n        .. versionadded:: 1.3\n\n    ``tabsize``\n        If given and greater than 0, expand tabs in the input (default: 0).\n    ``encoding``\n        If given, must be an encoding name. This encoding will be used to\n        convert the input string to Unicode, if it is not already a Unicode\n        string (default: ``'guess'``, which uses a simple UTF-8 / Locale /\n        Latin1 detection.  Can also be ``'chardet'`` to use the chardet\n        library, if it is installed.\n    ``inencoding``\n        Overrides the ``encoding`` if given.\n    \"\"\"\n\n    #: Full name of the lexer, in human-readable form\n    name = None\n\n    #: A list of short, unique identifiers that can be used to look\n    #: up the lexer from a list, e.g., using `get_lexer_by_name()`.\n    aliases = []\n\n    #: A list of `fnmatch` patterns that match filenames which contain\n    #: content for this lexer. The patterns in this list should be unique among\n    #: all lexers.\n    filenames = []\n\n    #: A list of `fnmatch` patterns that match filenames which may or may not\n    #: contain content for this lexer. This list is used by the\n    #: :func:`.guess_lexer_for_filename()` function, to determine which lexers\n    #: are then included in guessing the correct one. That means that\n    #: e.g. every lexer for HTML and a template language should include\n    #: ``\\*.html`` in this list.\n    alias_filenames = []\n\n    #: A list of MIME types for content that can be lexed with this lexer.\n    mimetypes = []\n\n    #: Priority, should multiple lexers match and no content is provided\n    priority = 0\n\n    #: URL of the language specification/definition. Used in the Pygments\n    #: documentation.\n    url = None\n\n    def __init__(self, **options):\n        \"\"\"\n        This constructor takes arbitrary options as keyword arguments.\n        Every subclass must first process its own options and then call\n        the `Lexer` constructor, since it processes the basic\n        options like `stripnl`.\n\n        An example looks like this:\n\n        .. sourcecode:: python\n\n           def __init__(self, **options):\n               self.compress = options.get('compress', '')\n               Lexer.__init__(self, **options)\n\n        As these options must all be specifiable as strings (due to the\n        command line usage), there are various utility functions\n        available to help with that, see `Utilities`_.\n        \"\"\"\n        self.options = options\n        self.stripnl = get_bool_opt(options, 'stripnl', True)\n        self.stripall = get_bool_opt(options, 'stripall', False)\n        self.ensurenl = get_bool_opt(options, 'ensurenl', True)\n        self.tabsize = get_int_opt(options, 'tabsize', 0)\n        self.encoding = options.get('encoding', 'guess')\n        self.encoding = options.get('inencoding') or self.encoding\n        self.filters = []\n        for filter_ in get_list_opt(options, 'filters', ()):\n            self.add_filter(filter_)\n\n    def __repr__(self):\n        if self.options:\n            return '<pygments.lexers.%s with %r>' % (self.__class__.__name__,\n                                                     self.options)\n        else:\n            return '<pygments.lexers.%s>' % self.__class__.__name__\n\n    def add_filter(self, filter_, **options):\n        \"\"\"\n        Add a new stream filter to this lexer.\n        \"\"\"\n        if not isinstance(filter_, Filter):\n            filter_ = get_filter_by_name(filter_, **options)\n        self.filters.append(filter_)\n\n    def analyse_text(text):\n        \"\"\"\n        A static method which is called for lexer guessing.\n\n        It should analyse the text and return a float in the range\n        from ``0.0`` to ``1.0``.  If it returns ``0.0``, the lexer\n        will not be selected as the most probable one, if it returns\n        ``1.0``, it will be selected immediately.  This is used by\n        `guess_lexer`.\n\n        The `LexerMeta` metaclass automatically wraps this function so\n        that it works like a static method (no ``self`` or ``cls``\n        parameter) and the return value is automatically converted to\n        `float`. If the return value is an object that is boolean `False`\n        it's the same as if the return values was ``0.0``.\n        \"\"\"\n\n    def get_tokens(self, text, unfiltered=False):\n        \"\"\"\n        This method is the basic interface of a lexer. It is called by\n        the `highlight()` function. It must process the text and return an\n        iterable of ``(tokentype, value)`` pairs from `text`.\n\n        Normally, you don't need to override this method. The default\n        implementation processes the options recognized by all lexers\n        (`stripnl`, `stripall` and so on), and then yields all tokens\n        from `get_tokens_unprocessed()`, with the ``index`` dropped.\n\n        If `unfiltered` is set to `True`, the filtering mechanism is\n        bypassed even if filters are defined.\n        \"\"\"\n        if not isinstance(text, str):\n            if self.encoding == 'guess':\n                text, _ = guess_decode(text)\n            elif self.encoding == 'chardet':\n                try:\n                    from pip._vendor import chardet\n                except ImportError as e:\n                    raise ImportError('To enable chardet encoding guessing, '\n                                      'please install the chardet library '\n                                      'from http://chardet.feedparser.org/') from e\n                # check for BOM first\n                decoded = None\n                for bom, encoding in _encoding_map:\n                    if text.startswith(bom):\n                        decoded = text[len(bom):].decode(encoding, 'replace')\n                        break\n                # no BOM found, so use chardet\n                if decoded is None:\n                    enc = chardet.detect(text[:1024])  # Guess using first 1KB\n                    decoded = text.decode(enc.get('encoding') or 'utf-8',\n                                          'replace')\n                text = decoded\n            else:\n                text = text.decode(self.encoding)\n                if text.startswith('\\ufeff'):\n                    text = text[len('\\ufeff'):]\n        else:\n            if text.startswith('\\ufeff'):\n                text = text[len('\\ufeff'):]\n\n        # text now *is* a unicode string\n        text = text.replace('\\r\\n', '\\n')\n        text = text.replace('\\r', '\\n')\n        if self.stripall:\n            text = text.strip()\n        elif self.stripnl:\n            text = text.strip('\\n')\n        if self.tabsize > 0:\n            text = text.expandtabs(self.tabsize)\n        if self.ensurenl and not text.endswith('\\n'):\n            text += '\\n'\n\n        def streamer():\n            for _, t, v in self.get_tokens_unprocessed(text):\n                yield t, v\n        stream = streamer()\n        if not unfiltered:\n            stream = apply_filters(stream, self.filters, self)\n        return stream\n\n    def get_tokens_unprocessed(self, text):\n        \"\"\"\n        This method should process the text and return an iterable of\n        ``(index, tokentype, value)`` tuples where ``index`` is the starting\n        position of the token within the input text.\n\n        It must be overridden by subclasses. It is recommended to\n        implement it as a generator to maximize effectiveness.\n        \"\"\"\n        raise NotImplementedError\n\n\nclass DelegatingLexer(Lexer):\n    \"\"\"\n    This lexer takes two lexer as arguments. A root lexer and\n    a language lexer. First everything is scanned using the language\n    lexer, afterwards all ``Other`` tokens are lexed using the root\n    lexer.\n\n    The lexers from the ``template`` lexer package use this base lexer.\n    \"\"\"\n\n    def __init__(self, _root_lexer, _language_lexer, _needle=Other, **options):\n        self.root_lexer = _root_lexer(**options)\n        self.language_lexer = _language_lexer(**options)\n        self.needle = _needle\n        Lexer.__init__(self, **options)\n\n    def get_tokens_unprocessed(self, text):\n        buffered = ''\n        insertions = []\n        lng_buffer = []\n        for i, t, v in self.language_lexer.get_tokens_unprocessed(text):\n            if t is self.needle:\n                if lng_buffer:\n                    insertions.append((len(buffered), lng_buffer))\n                    lng_buffer = []\n                buffered += v\n            else:\n                lng_buffer.append((i, t, v))\n        if lng_buffer:\n            insertions.append((len(buffered), lng_buffer))\n        return do_insertions(insertions,\n                             self.root_lexer.get_tokens_unprocessed(buffered))\n\n\n# ------------------------------------------------------------------------------\n# RegexLexer and ExtendedRegexLexer\n#\n\n\nclass include(str):  # pylint: disable=invalid-name\n    \"\"\"\n    Indicates that a state should include rules from another state.\n    \"\"\"\n    pass\n\n\nclass _inherit:\n    \"\"\"\n    Indicates the a state should inherit from its superclass.\n    \"\"\"\n    def __repr__(self):\n        return 'inherit'\n\ninherit = _inherit()  # pylint: disable=invalid-name\n\n\nclass combined(tuple):  # pylint: disable=invalid-name\n    \"\"\"\n    Indicates a state combined from multiple states.\n    \"\"\"\n\n    def __new__(cls, *args):\n        return tuple.__new__(cls, args)\n\n    def __init__(self, *args):\n        # tuple.__init__ doesn't do anything\n        pass\n\n\nclass _PseudoMatch:\n    \"\"\"\n    A pseudo match object constructed from a string.\n    \"\"\"\n\n    def __init__(self, start, text):\n        self._text = text\n        self._start = start\n\n    def start(self, arg=None):\n        return self._start\n\n    def end(self, arg=None):\n        return self._start + len(self._text)\n\n    def group(self, arg=None):\n        if arg:\n            raise IndexError('No such group')\n        return self._text\n\n    def groups(self):\n        return (self._text,)\n\n    def groupdict(self):\n        return {}\n\n\ndef bygroups(*args):\n    \"\"\"\n    Callback that yields multiple actions for each group in the match.\n    \"\"\"\n    def callback(lexer, match, ctx=None):\n        for i, action in enumerate(args):\n            if action is None:\n                continue\n            elif type(action) is _TokenType:\n                data = match.group(i + 1)\n                if data:\n                    yield match.start(i + 1), action, data\n            else:\n                data = match.group(i + 1)\n                if data is not None:\n                    if ctx:\n                        ctx.pos = match.start(i + 1)\n                    for item in action(lexer,\n                                       _PseudoMatch(match.start(i + 1), data), ctx):\n                        if item:\n                            yield item\n        if ctx:\n            ctx.pos = match.end()\n    return callback\n\n\nclass _This:\n    \"\"\"\n    Special singleton used for indicating the caller class.\n    Used by ``using``.\n    \"\"\"\n\nthis = _This()\n\n\ndef using(_other, **kwargs):\n    \"\"\"\n    Callback that processes the match with a different lexer.\n\n    The keyword arguments are forwarded to the lexer, except `state` which\n    is handled separately.\n\n    `state` specifies the state that the new lexer will start in, and can\n    be an enumerable such as ('root', 'inline', 'string') or a simple\n    string which is assumed to be on top of the root state.\n\n    Note: For that to work, `_other` must not be an `ExtendedRegexLexer`.\n    \"\"\"\n    gt_kwargs = {}\n    if 'state' in kwargs:\n        s = kwargs.pop('state')\n        if isinstance(s, (list, tuple)):\n            gt_kwargs['stack'] = s\n        else:\n            gt_kwargs['stack'] = ('root', s)\n\n    if _other is this:\n        def callback(lexer, match, ctx=None):\n            # if keyword arguments are given the callback\n            # function has to create a new lexer instance\n            if kwargs:\n                # XXX: cache that somehow\n                kwargs.update(lexer.options)\n                lx = lexer.__class__(**kwargs)\n            else:\n                lx = lexer\n            s = match.start()\n            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):\n                yield i + s, t, v\n            if ctx:\n                ctx.pos = match.end()\n    else:\n        def callback(lexer, match, ctx=None):\n            # XXX: cache that somehow\n            kwargs.update(lexer.options)\n            lx = _other(**kwargs)\n\n            s = match.start()\n            for i, t, v in lx.get_tokens_unprocessed(match.group(), **gt_kwargs):\n                yield i + s, t, v\n            if ctx:\n                ctx.pos = match.end()\n    return callback\n\n\nclass default:\n    \"\"\"\n    Indicates a state or state action (e.g. #pop) to apply.\n    For example default('#pop') is equivalent to ('', Token, '#pop')\n    Note that state tuples may be used as well.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    def __init__(self, state):\n        self.state = state\n\n\nclass words(Future):\n    \"\"\"\n    Indicates a list of literal words that is transformed into an optimized\n    regex that matches any of the words.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    def __init__(self, words, prefix='', suffix=''):\n        self.words = words\n        self.prefix = prefix\n        self.suffix = suffix\n\n    def get(self):\n        return regex_opt(self.words, prefix=self.prefix, suffix=self.suffix)\n\n\nclass RegexLexerMeta(LexerMeta):\n    \"\"\"\n    Metaclass for RegexLexer, creates the self._tokens attribute from\n    self.tokens on the first instantiation.\n    \"\"\"\n\n    def _process_regex(cls, regex, rflags, state):\n        \"\"\"Preprocess the regular expression component of a token definition.\"\"\"\n        if isinstance(regex, Future):\n            regex = regex.get()\n        return re.compile(regex, rflags).match\n\n    def _process_token(cls, token):\n        \"\"\"Preprocess the token component of a token definition.\"\"\"\n        assert type(token) is _TokenType or callable(token), \\\n            'token type must be simple type or callable, not %r' % (token,)\n        return token\n\n    def _process_new_state(cls, new_state, unprocessed, processed):\n        \"\"\"Preprocess the state transition action of a token definition.\"\"\"\n        if isinstance(new_state, str):\n            # an existing state\n            if new_state == '#pop':\n                return -1\n            elif new_state in unprocessed:\n                return (new_state,)\n            elif new_state == '#push':\n                return new_state\n            elif new_state[:5] == '#pop:':\n                return -int(new_state[5:])\n            else:\n                assert False, 'unknown new state %r' % new_state\n        elif isinstance(new_state, combined):\n            # combine a new state from existing ones\n            tmp_state = '_tmp_%d' % cls._tmpname\n            cls._tmpname += 1\n            itokens = []\n            for istate in new_state:\n                assert istate != new_state, 'circular state ref %r' % istate\n                itokens.extend(cls._process_state(unprocessed,\n                                                  processed, istate))\n            processed[tmp_state] = itokens\n            return (tmp_state,)\n        elif isinstance(new_state, tuple):\n            # push more than one state\n            for istate in new_state:\n                assert (istate in unprocessed or\n                        istate in ('#pop', '#push')), \\\n                    'unknown new state ' + istate\n            return new_state\n        else:\n            assert False, 'unknown new state def %r' % new_state\n\n    def _process_state(cls, unprocessed, processed, state):\n        \"\"\"Preprocess a single state definition.\"\"\"\n        assert type(state) is str, \"wrong state name %r\" % state\n        assert state[0] != '#', \"invalid state name %r\" % state\n        if state in processed:\n            return processed[state]\n        tokens = processed[state] = []\n        rflags = cls.flags\n        for tdef in unprocessed[state]:\n            if isinstance(tdef, include):\n                # it's a state reference\n                assert tdef != state, \"circular state reference %r\" % state\n                tokens.extend(cls._process_state(unprocessed, processed,\n                                                 str(tdef)))\n                continue\n            if isinstance(tdef, _inherit):\n                # should be processed already, but may not in the case of:\n                # 1. the state has no counterpart in any parent\n                # 2. the state includes more than one 'inherit'\n                continue\n            if isinstance(tdef, default):\n                new_state = cls._process_new_state(tdef.state, unprocessed, processed)\n                tokens.append((re.compile('').match, None, new_state))\n                continue\n\n            assert type(tdef) is tuple, \"wrong rule def %r\" % tdef\n\n            try:\n                rex = cls._process_regex(tdef[0], rflags, state)\n            except Exception as err:\n                raise ValueError(\"uncompilable regex %r in state %r of %r: %s\" %\n                                 (tdef[0], state, cls, err)) from err\n\n            token = cls._process_token(tdef[1])\n\n            if len(tdef) == 2:\n                new_state = None\n            else:\n                new_state = cls._process_new_state(tdef[2],\n                                                   unprocessed, processed)\n\n            tokens.append((rex, token, new_state))\n        return tokens\n\n    def process_tokendef(cls, name, tokendefs=None):\n        \"\"\"Preprocess a dictionary of token definitions.\"\"\"\n        processed = cls._all_tokens[name] = {}\n        tokendefs = tokendefs or cls.tokens[name]\n        for state in list(tokendefs):\n            cls._process_state(tokendefs, processed, state)\n        return processed\n\n    def get_tokendefs(cls):\n        \"\"\"\n        Merge tokens from superclasses in MRO order, returning a single tokendef\n        dictionary.\n\n        Any state that is not defined by a subclass will be inherited\n        automatically.  States that *are* defined by subclasses will, by\n        default, override that state in the superclass.  If a subclass wishes to\n        inherit definitions from a superclass, it can use the special value\n        \"inherit\", which will cause the superclass' state definition to be\n        included at that point in the state.\n        \"\"\"\n        tokens = {}\n        inheritable = {}\n        for c in cls.__mro__:\n            toks = c.__dict__.get('tokens', {})\n\n            for state, items in toks.items():\n                curitems = tokens.get(state)\n                if curitems is None:\n                    # N.b. because this is assigned by reference, sufficiently\n                    # deep hierarchies are processed incrementally (e.g. for\n                    # A(B), B(C), C(RegexLexer), B will be premodified so X(B)\n                    # will not see any inherits in B).\n                    tokens[state] = items\n                    try:\n                        inherit_ndx = items.index(inherit)\n                    except ValueError:\n                        continue\n                    inheritable[state] = inherit_ndx\n                    continue\n\n                inherit_ndx = inheritable.pop(state, None)\n                if inherit_ndx is None:\n                    continue\n\n                # Replace the \"inherit\" value with the items\n                curitems[inherit_ndx:inherit_ndx+1] = items\n                try:\n                    # N.b. this is the index in items (that is, the superclass\n                    # copy), so offset required when storing below.\n                    new_inh_ndx = items.index(inherit)\n                except ValueError:\n                    pass\n                else:\n                    inheritable[state] = inherit_ndx + new_inh_ndx\n\n        return tokens\n\n    def __call__(cls, *args, **kwds):\n        \"\"\"Instantiate cls after preprocessing its token definitions.\"\"\"\n        if '_tokens' not in cls.__dict__:\n            cls._all_tokens = {}\n            cls._tmpname = 0\n            if hasattr(cls, 'token_variants') and cls.token_variants:\n                # don't process yet\n                pass\n            else:\n                cls._tokens = cls.process_tokendef('', cls.get_tokendefs())\n\n        return type.__call__(cls, *args, **kwds)\n\n\nclass RegexLexer(Lexer, metaclass=RegexLexerMeta):\n    \"\"\"\n    Base for simple stateful regular expression-based lexers.\n    Simplifies the lexing process so that you need only\n    provide a list of states and regular expressions.\n    \"\"\"\n\n    #: Flags for compiling the regular expressions.\n    #: Defaults to MULTILINE.\n    flags = re.MULTILINE\n\n    #: At all time there is a stack of states. Initially, the stack contains\n    #: a single state 'root'. The top of the stack is called \"the current state\".\n    #:\n    #: Dict of ``{'state': [(regex, tokentype, new_state), ...], ...}``\n    #:\n    #: ``new_state`` can be omitted to signify no state transition.\n    #: If ``new_state`` is a string, it is pushed on the stack. This ensure\n    #: the new current state is ``new_state``.\n    #: If ``new_state`` is a tuple of strings, all of those strings are pushed\n    #: on the stack and the current state will be the last element of the list.\n    #: ``new_state`` can also be ``combined('state1', 'state2', ...)``\n    #: to signify a new, anonymous state combined from the rules of two\n    #: or more existing ones.\n    #: Furthermore, it can be '#pop' to signify going back one step in\n    #: the state stack, or '#push' to push the current state on the stack\n    #: again. Note that if you push while in a combined state, the combined\n    #: state itself is pushed, and not only the state in which the rule is\n    #: defined.\n    #:\n    #: The tuple can also be replaced with ``include('state')``, in which\n    #: case the rules from the state named by the string are included in the\n    #: current one.\n    tokens = {}\n\n    def get_tokens_unprocessed(self, text, stack=('root',)):\n        \"\"\"\n        Split ``text`` into (tokentype, text) pairs.\n\n        ``stack`` is the initial stack (default: ``['root']``)\n        \"\"\"\n        pos = 0\n        tokendefs = self._tokens\n        statestack = list(stack)\n        statetokens = tokendefs[statestack[-1]]\n        while 1:\n            for rexmatch, action, new_state in statetokens:\n                m = rexmatch(text, pos)\n                if m:\n                    if action is not None:\n                        if type(action) is _TokenType:\n                            yield pos, action, m.group()\n                        else:\n                            yield from action(self, m)\n                    pos = m.end()\n                    if new_state is not None:\n                        # state transition\n                        if isinstance(new_state, tuple):\n                            for state in new_state:\n                                if state == '#pop':\n                                    if len(statestack) > 1:\n                                        statestack.pop()\n                                elif state == '#push':\n                                    statestack.append(statestack[-1])\n                                else:\n                                    statestack.append(state)\n                        elif isinstance(new_state, int):\n                            # pop, but keep at least one state on the stack\n                            # (random code leading to unexpected pops should\n                            # not allow exceptions)\n                            if abs(new_state) >= len(statestack):\n                                del statestack[1:]\n                            else:\n                                del statestack[new_state:]\n                        elif new_state == '#push':\n                            statestack.append(statestack[-1])\n                        else:\n                            assert False, \"wrong state def: %r\" % new_state\n                        statetokens = tokendefs[statestack[-1]]\n                    break\n            else:\n                # We are here only if all state tokens have been considered\n                # and there was not a match on any of them.\n                try:\n                    if text[pos] == '\\n':\n                        # at EOL, reset state to \"root\"\n                        statestack = ['root']\n                        statetokens = tokendefs['root']\n                        yield pos, Whitespace, '\\n'\n                        pos += 1\n                        continue\n                    yield pos, Error, text[pos]\n                    pos += 1\n                except IndexError:\n                    break\n\n\nclass LexerContext:\n    \"\"\"\n    A helper object that holds lexer position data.\n    \"\"\"\n\n    def __init__(self, text, pos, stack=None, end=None):\n        self.text = text\n        self.pos = pos\n        self.end = end or len(text)  # end=0 not supported ;-)\n        self.stack = stack or ['root']\n\n    def __repr__(self):\n        return 'LexerContext(%r, %r, %r)' % (\n            self.text, self.pos, self.stack)\n\n\nclass ExtendedRegexLexer(RegexLexer):\n    \"\"\"\n    A RegexLexer that uses a context object to store its state.\n    \"\"\"\n\n    def get_tokens_unprocessed(self, text=None, context=None):\n        \"\"\"\n        Split ``text`` into (tokentype, text) pairs.\n        If ``context`` is given, use this lexer context instead.\n        \"\"\"\n        tokendefs = self._tokens\n        if not context:\n            ctx = LexerContext(text, 0)\n            statetokens = tokendefs['root']\n        else:\n            ctx = context\n            statetokens = tokendefs[ctx.stack[-1]]\n            text = ctx.text\n        while 1:\n            for rexmatch, action, new_state in statetokens:\n                m = rexmatch(text, ctx.pos, ctx.end)\n                if m:\n                    if action is not None:\n                        if type(action) is _TokenType:\n                            yield ctx.pos, action, m.group()\n                            ctx.pos = m.end()\n                        else:\n                            yield from action(self, m, ctx)\n                            if not new_state:\n                                # altered the state stack?\n                                statetokens = tokendefs[ctx.stack[-1]]\n                    # CAUTION: callback must set ctx.pos!\n                    if new_state is not None:\n                        # state transition\n                        if isinstance(new_state, tuple):\n                            for state in new_state:\n                                if state == '#pop':\n                                    if len(ctx.stack) > 1:\n                                        ctx.stack.pop()\n                                elif state == '#push':\n                                    ctx.stack.append(ctx.stack[-1])\n                                else:\n                                    ctx.stack.append(state)\n                        elif isinstance(new_state, int):\n                            # see RegexLexer for why this check is made\n                            if abs(new_state) >= len(ctx.stack):\n                                del ctx.stack[1:]\n                            else:\n                                del ctx.stack[new_state:]\n                        elif new_state == '#push':\n                            ctx.stack.append(ctx.stack[-1])\n                        else:\n                            assert False, \"wrong state def: %r\" % new_state\n                        statetokens = tokendefs[ctx.stack[-1]]\n                    break\n            else:\n                try:\n                    if ctx.pos >= ctx.end:\n                        break\n                    if text[ctx.pos] == '\\n':\n                        # at EOL, reset state to \"root\"\n                        ctx.stack = ['root']\n                        statetokens = tokendefs['root']\n                        yield ctx.pos, Text, '\\n'\n                        ctx.pos += 1\n                        continue\n                    yield ctx.pos, Error, text[ctx.pos]\n                    ctx.pos += 1\n                except IndexError:\n                    break\n\n\ndef do_insertions(insertions, tokens):\n    \"\"\"\n    Helper for lexers which must combine the results of several\n    sublexers.\n\n    ``insertions`` is a list of ``(index, itokens)`` pairs.\n    Each ``itokens`` iterable should be inserted at position\n    ``index`` into the token stream given by the ``tokens``\n    argument.\n\n    The result is a combined token stream.\n\n    TODO: clean up the code here.\n    \"\"\"\n    insertions = iter(insertions)\n    try:\n        index, itokens = next(insertions)\n    except StopIteration:\n        # no insertions\n        yield from tokens\n        return\n\n    realpos = None\n    insleft = True\n\n    # iterate over the token stream where we want to insert\n    # the tokens from the insertion list.\n    for i, t, v in tokens:\n        # first iteration. store the position of first item\n        if realpos is None:\n            realpos = i\n        oldi = 0\n        while insleft and i + len(v) >= index:\n            tmpval = v[oldi:index - i]\n            if tmpval:\n                yield realpos, t, tmpval\n                realpos += len(tmpval)\n            for it_index, it_token, it_value in itokens:\n                yield realpos, it_token, it_value\n                realpos += len(it_value)\n            oldi = index - i\n            try:\n                index, itokens = next(insertions)\n            except StopIteration:\n                insleft = False\n                break  # not strictly necessary\n        if oldi < len(v):\n            yield realpos, t, v[oldi:]\n            realpos += len(v) - oldi\n\n    # leftover tokens\n    while insleft:\n        # no normal tokens, set realpos to zero\n        realpos = realpos or 0\n        for p, t, v in itokens:\n            yield realpos, t, v\n            realpos += len(v)\n        try:\n            index, itokens = next(insertions)\n        except StopIteration:\n            insleft = False\n            break  # not strictly necessary\n\n\nclass ProfilingRegexLexerMeta(RegexLexerMeta):\n    \"\"\"Metaclass for ProfilingRegexLexer, collects regex timing info.\"\"\"\n\n    def _process_regex(cls, regex, rflags, state):\n        if isinstance(regex, words):\n            rex = regex_opt(regex.words, prefix=regex.prefix,\n                            suffix=regex.suffix)\n        else:\n            rex = regex\n        compiled = re.compile(rex, rflags)\n\n        def match_func(text, pos, endpos=sys.maxsize):\n            info = cls._prof_data[-1].setdefault((state, rex), [0, 0.0])\n            t0 = time.time()\n            res = compiled.match(text, pos, endpos)\n            t1 = time.time()\n            info[0] += 1\n            info[1] += t1 - t0\n            return res\n        return match_func\n\n\nclass ProfilingRegexLexer(RegexLexer, metaclass=ProfilingRegexLexerMeta):\n    \"\"\"Drop-in replacement for RegexLexer that does profiling of its regexes.\"\"\"\n\n    _prof_data = []\n    _prof_sort_index = 4  # defaults to time per call\n\n    def get_tokens_unprocessed(self, text, stack=('root',)):\n        # this needs to be a stack, since using(this) will produce nested calls\n        self.__class__._prof_data.append({})\n        yield from RegexLexer.get_tokens_unprocessed(self, text, stack)\n        rawdata = self.__class__._prof_data.pop()\n        data = sorted(((s, repr(r).strip('u\\'').replace('\\\\\\\\', '\\\\')[:65],\n                        n, 1000 * t, 1000 * t / n)\n                       for ((s, r), (n, t)) in rawdata.items()),\n                      key=lambda x: x[self._prof_sort_index],\n                      reverse=True)\n        sum_total = sum(x[3] for x in data)\n\n        print()\n        print('Profiling result for %s lexing %d chars in %.3f ms' %\n              (self.__class__.__name__, len(text), sum_total))\n        print('=' * 110)\n        print('%-20s %-64s ncalls  tottime  percall' % ('state', 'regex'))\n        print('-' * 110)\n        for d in data:\n            print('%-20s %-65s %5d %8.4f %8.4f' % d)\n        print('=' * 110)\n"},"hash":"b6ym6QSY4P"}