{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:pip:_vendor:pyparsing:helpers.py","body":"# helpers.py\nimport html.entities\nimport re\nimport sys\nimport typing\n\nfrom . import __diag__\nfrom .core import *\nfrom .util import (\n    _bslash,\n    _flatten,\n    _escape_regex_range_chars,\n    replaced_by_pep8,\n)\n\n\n#\n# global helpers\n#\ndef counted_array(\n    expr: ParserElement,\n    int_expr: typing.Optional[ParserElement] = None,\n    *,\n    intExpr: typing.Optional[ParserElement] = None,\n) -> ParserElement:\n    \"\"\"Helper to define a counted list of expressions.\n\n    This helper defines a pattern of the form::\n\n        integer expr expr expr...\n\n    where the leading integer tells how many expr expressions follow.\n    The matched tokens returns the array of expr tokens as a list - the\n    leading count token is suppressed.\n\n    If ``int_expr`` is specified, it should be a pyparsing expression\n    that produces an integer value.\n\n    Example::\n\n        counted_array(Word(alphas)).parse_string('2 ab cd ef')  # -> ['ab', 'cd']\n\n        # in this parser, the leading integer value is given in binary,\n        # '10' indicating that 2 values are in the array\n        binary_constant = Word('01').set_parse_action(lambda t: int(t[0], 2))\n        counted_array(Word(alphas), int_expr=binary_constant).parse_string('10 ab cd ef')  # -> ['ab', 'cd']\n\n        # if other fields must be parsed after the count but before the\n        # list items, give the fields results names and they will\n        # be preserved in the returned ParseResults:\n        count_with_metadata = integer + Word(alphas)(\"type\")\n        typed_array = counted_array(Word(alphanums), int_expr=count_with_metadata)(\"items\")\n        result = typed_array.parse_string(\"3 bool True True False\")\n        print(result.dump())\n\n        # prints\n        # ['True', 'True', 'False']\n        # - items: ['True', 'True', 'False']\n        # - type: 'bool'\n    \"\"\"\n    intExpr = intExpr or int_expr\n    array_expr = Forward()\n\n    def count_field_parse_action(s, l, t):\n        nonlocal array_expr\n        n = t[0]\n        array_expr <<= (expr * n) if n else Empty()\n        # clear list contents, but keep any named results\n        del t[:]\n\n    if intExpr is None:\n        intExpr = Word(nums).set_parse_action(lambda t: int(t[0]))\n    else:\n        intExpr = intExpr.copy()\n    intExpr.set_name(\"arrayLen\")\n    intExpr.add_parse_action(count_field_parse_action, call_during_try=True)\n    return (intExpr + array_expr).set_name(\"(len) \" + str(expr) + \"...\")\n\n\ndef match_previous_literal(expr: ParserElement) -> ParserElement:\n    \"\"\"Helper to define an expression that is indirectly defined from\n    the tokens matched in a previous expression, that is, it looks for\n    a 'repeat' of a previous expression.  For example::\n\n        first = Word(nums)\n        second = match_previous_literal(first)\n        match_expr = first + \":\" + second\n\n    will match ``\"1:1\"``, but not ``\"1:2\"``.  Because this\n    matches a previous literal, will also match the leading\n    ``\"1:1\"`` in ``\"1:10\"``. If this is not desired, use\n    :class:`match_previous_expr`. Do *not* use with packrat parsing\n    enabled.\n    \"\"\"\n    rep = Forward()\n\n    def copy_token_to_repeater(s, l, t):\n        if t:\n            if len(t) == 1:\n                rep << t[0]\n            else:\n                # flatten t tokens\n                tflat = _flatten(t.as_list())\n                rep << And(Literal(tt) for tt in tflat)\n        else:\n            rep << Empty()\n\n    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)\n    rep.set_name(\"(prev) \" + str(expr))\n    return rep\n\n\ndef match_previous_expr(expr: ParserElement) -> ParserElement:\n    \"\"\"Helper to define an expression that is indirectly defined from\n    the tokens matched in a previous expression, that is, it looks for\n    a 'repeat' of a previous expression.  For example::\n\n        first = Word(nums)\n        second = match_previous_expr(first)\n        match_expr = first + \":\" + second\n\n    will match ``\"1:1\"``, but not ``\"1:2\"``.  Because this\n    matches by expressions, will *not* match the leading ``\"1:1\"``\n    in ``\"1:10\"``; the expressions are evaluated first, and then\n    compared, so ``\"1\"`` is compared with ``\"10\"``. Do *not* use\n    with packrat parsing enabled.\n    \"\"\"\n    rep = Forward()\n    e2 = expr.copy()\n    rep <<= e2\n\n    def copy_token_to_repeater(s, l, t):\n        matchTokens = _flatten(t.as_list())\n\n        def must_match_these_tokens(s, l, t):\n            theseTokens = _flatten(t.as_list())\n            if theseTokens != matchTokens:\n                raise ParseException(\n                    s, l, f\"Expected {matchTokens}, found{theseTokens}\"\n                )\n\n        rep.set_parse_action(must_match_these_tokens, callDuringTry=True)\n\n    expr.add_parse_action(copy_token_to_repeater, callDuringTry=True)\n    rep.set_name(\"(prev) \" + str(expr))\n    return rep\n\n\ndef one_of(\n    strs: Union[typing.Iterable[str], str],\n    caseless: bool = False,\n    use_regex: bool = True,\n    as_keyword: bool = False,\n    *,\n    useRegex: bool = True,\n    asKeyword: bool = False,\n) -> ParserElement:\n    \"\"\"Helper to quickly define a set of alternative :class:`Literal` s,\n    and makes sure to do longest-first testing when there is a conflict,\n    regardless of the input order, but returns\n    a :class:`MatchFirst` for best performance.\n\n    Parameters:\n\n    - ``strs`` - a string of space-delimited literals, or a collection of\n      string literals\n    - ``caseless`` - treat all literals as caseless - (default= ``False``)\n    - ``use_regex`` - as an optimization, will\n      generate a :class:`Regex` object; otherwise, will generate\n      a :class:`MatchFirst` object (if ``caseless=True`` or ``as_keyword=True``, or if\n      creating a :class:`Regex` raises an exception) - (default= ``True``)\n    - ``as_keyword`` - enforce :class:`Keyword`-style matching on the\n      generated expressions - (default= ``False``)\n    - ``asKeyword`` and ``useRegex`` are retained for pre-PEP8 compatibility,\n      but will be removed in a future release\n\n    Example::\n\n        comp_oper = one_of(\"< = > <= >= !=\")\n        var = Word(alphas)\n        number = Word(nums)\n        term = var | number\n        comparison_expr = term + comp_oper + term\n        print(comparison_expr.search_string(\"B = 12  AA=23 B<=AA AA>12\"))\n\n    prints::\n\n        [['B', '=', '12'], ['AA', '=', '23'], ['B', '<=', 'AA'], ['AA', '>', '12']]\n    \"\"\"\n    asKeyword = asKeyword or as_keyword\n    useRegex = useRegex and use_regex\n\n    if (\n        isinstance(caseless, str_type)\n        and __diag__.warn_on_multiple_string_args_to_oneof\n    ):\n        warnings.warn(\n            \"More than one string argument passed to one_of, pass\"\n            \" choices as a list or space-delimited string\",\n            stacklevel=2,\n        )\n\n    if caseless:\n        isequal = lambda a, b: a.upper() == b.upper()\n        masks = lambda a, b: b.upper().startswith(a.upper())\n        parseElementClass = CaselessKeyword if asKeyword else CaselessLiteral\n    else:\n        isequal = lambda a, b: a == b\n        masks = lambda a, b: b.startswith(a)\n        parseElementClass = Keyword if asKeyword else Literal\n\n    symbols: List[str] = []\n    if isinstance(strs, str_type):\n        strs = typing.cast(str, strs)\n        symbols = strs.split()\n    elif isinstance(strs, Iterable):\n        symbols = list(strs)\n    else:\n        raise TypeError(\"Invalid argument to one_of, expected string or iterable\")\n    if not symbols:\n        return NoMatch()\n\n    # reorder given symbols to take care to avoid masking longer choices with shorter ones\n    # (but only if the given symbols are not just single characters)\n    if any(len(sym) > 1 for sym in symbols):\n        i = 0\n        while i < len(symbols) - 1:\n            cur = symbols[i]\n            for j, other in enumerate(symbols[i + 1 :]):\n                if isequal(other, cur):\n                    del symbols[i + j + 1]\n                    break\n                elif masks(cur, other):\n                    del symbols[i + j + 1]\n                    symbols.insert(i, other)\n                    break\n            else:\n                i += 1\n\n    if useRegex:\n        re_flags: int = re.IGNORECASE if caseless else 0\n\n        try:\n            if all(len(sym) == 1 for sym in symbols):\n                # symbols are just single characters, create range regex pattern\n                patt = f\"[{''.join(_escape_regex_range_chars(sym) for sym in symbols)}]\"\n            else:\n                patt = \"|\".join(re.escape(sym) for sym in symbols)\n\n            # wrap with \\b word break markers if defining as keywords\n            if asKeyword:\n                patt = rf\"\\b(?:{patt})\\b\"\n\n            ret = Regex(patt, flags=re_flags).set_name(\" | \".join(symbols))\n\n            if caseless:\n                # add parse action to return symbols as specified, not in random\n                # casing as found in input string\n                symbol_map = {sym.lower(): sym for sym in symbols}\n                ret.add_parse_action(lambda s, l, t: symbol_map[t[0].lower()])\n\n            return ret\n\n        except re.error:\n            warnings.warn(\n                \"Exception creating Regex for one_of, building MatchFirst\", stacklevel=2\n            )\n\n    # last resort, just use MatchFirst\n    return MatchFirst(parseElementClass(sym) for sym in symbols).set_name(\n        \" | \".join(symbols)\n    )\n\n\ndef dict_of(key: ParserElement, value: ParserElement) -> ParserElement:\n    \"\"\"Helper to easily and clearly define a dictionary by specifying\n    the respective patterns for the key and value.  Takes care of\n    defining the :class:`Dict`, :class:`ZeroOrMore`, and\n    :class:`Group` tokens in the proper order.  The key pattern\n    can include delimiting markers or punctuation, as long as they are\n    suppressed, thereby leaving the significant key text.  The value\n    pattern can include named results, so that the :class:`Dict` results\n    can include named token fields.\n\n    Example::\n\n        text = \"shape: SQUARE posn: upper left color: light blue texture: burlap\"\n        attr_expr = (label + Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join))\n        print(attr_expr[1, ...].parse_string(text).dump())\n\n        attr_label = label\n        attr_value = Suppress(':') + OneOrMore(data_word, stop_on=label).set_parse_action(' '.join)\n\n        # similar to Dict, but simpler call format\n        result = dict_of(attr_label, attr_value).parse_string(text)\n        print(result.dump())\n        print(result['shape'])\n        print(result.shape)  # object attribute access works too\n        print(result.as_dict())\n\n    prints::\n\n        [['shape', 'SQUARE'], ['posn', 'upper left'], ['color', 'light blue'], ['texture', 'burlap']]\n        - color: 'light blue'\n        - posn: 'upper left'\n        - shape: 'SQUARE'\n        - texture: 'burlap'\n        SQUARE\n        SQUARE\n        {'color': 'light blue', 'shape': 'SQUARE', 'posn': 'upper left', 'texture': 'burlap'}\n    \"\"\"\n    return Dict(OneOrMore(Group(key + value)))\n\n\ndef original_text_for(\n    expr: ParserElement, as_string: bool = True, *, asString: bool = True\n) -> ParserElement:\n    \"\"\"Helper to return the original, untokenized text for a given\n    expression.  Useful to restore the parsed fields of an HTML start\n    tag into the raw tag text itself, or to revert separate tokens with\n    intervening whitespace back to the original matching input text. By\n    default, returns a string containing the original parsed text.\n\n    If the optional ``as_string`` argument is passed as\n    ``False``, then the return value is\n    a :class:`ParseResults` containing any results names that\n    were originally matched, and a single token containing the original\n    matched text from the input string.  So if the expression passed to\n    :class:`original_text_for` contains expressions with defined\n    results names, you must set ``as_string`` to ``False`` if you\n    want to preserve those results name values.\n\n    The ``asString`` pre-PEP8 argument is retained for compatibility,\n    but will be removed in a future release.\n\n    Example::\n\n        src = \"this is test <b> bold <i>text</i> </b> normal text \"\n        for tag in (\"b\", \"i\"):\n            opener, closer = make_html_tags(tag)\n            patt = original_text_for(opener + ... + closer)\n            print(patt.search_string(src)[0])\n\n    prints::\n\n        ['<b> bold <i>text</i> </b>']\n        ['<i>text</i>']\n    \"\"\"\n    asString = asString and as_string\n\n    locMarker = Empty().set_parse_action(lambda s, loc, t: loc)\n    endlocMarker = locMarker.copy()\n    endlocMarker.callPreparse = False\n    matchExpr = locMarker(\"_original_start\") + expr + endlocMarker(\"_original_end\")\n    if asString:\n        extractText = lambda s, l, t: s[t._original_start : t._original_end]\n    else:\n\n        def extractText(s, l, t):\n            t[:] = [s[t.pop(\"_original_start\") : t.pop(\"_original_end\")]]\n\n    matchExpr.set_parse_action(extractText)\n    matchExpr.ignoreExprs = expr.ignoreExprs\n    matchExpr.suppress_warning(Diagnostics.warn_ungrouped_named_tokens_in_collection)\n    return matchExpr\n\n\ndef ungroup(expr: ParserElement) -> ParserElement:\n    \"\"\"Helper to undo pyparsing's default grouping of And expressions,\n    even if all but one are non-empty.\n    \"\"\"\n    return TokenConverter(expr).add_parse_action(lambda t: t[0])\n\n\ndef locatedExpr(expr: ParserElement) -> ParserElement:\n    \"\"\"\n    (DEPRECATED - future code should use the :class:`Located` class)\n    Helper to decorate a returned token with its starting and ending\n    locations in the input string.\n\n    This helper adds the following results names:\n\n    - ``locn_start`` - location where matched expression begins\n    - ``locn_end`` - location where matched expression ends\n    - ``value`` - the actual parsed results\n\n    Be careful if the input text contains ``<TAB>`` characters, you\n    may want to call :class:`ParserElement.parse_with_tabs`\n\n    Example::\n\n        wd = Word(alphas)\n        for match in locatedExpr(wd).search_string(\"ljsdf123lksdjjf123lkkjj1222\"):\n            print(match)\n\n    prints::\n\n        [[0, 'ljsdf', 5]]\n        [[8, 'lksdjjf', 15]]\n        [[18, 'lkkjj', 23]]\n    \"\"\"\n    locator = Empty().set_parse_action(lambda ss, ll, tt: ll)\n    return Group(\n        locator(\"locn_start\")\n        + expr(\"value\")\n        + locator.copy().leaveWhitespace()(\"locn_end\")\n    )\n\n\ndef nested_expr(\n    opener: Union[str, ParserElement] = \"(\",\n    closer: Union[str, ParserElement] = \")\",\n    content: typing.Optional[ParserElement] = None,\n    ignore_expr: ParserElement = quoted_string(),\n    *,\n    ignoreExpr: ParserElement = quoted_string(),\n) -> ParserElement:\n    \"\"\"Helper method for defining nested lists enclosed in opening and\n    closing delimiters (``\"(\"`` and ``\")\"`` are the default).\n\n    Parameters:\n\n    - ``opener`` - opening character for a nested list\n      (default= ``\"(\"``); can also be a pyparsing expression\n    - ``closer`` - closing character for a nested list\n      (default= ``\")\"``); can also be a pyparsing expression\n    - ``content`` - expression for items within the nested lists\n      (default= ``None``)\n    - ``ignore_expr`` - expression for ignoring opening and closing delimiters\n      (default= :class:`quoted_string`)\n    - ``ignoreExpr`` - this pre-PEP8 argument is retained for compatibility\n      but will be removed in a future release\n\n    If an expression is not provided for the content argument, the\n    nested expression will capture all whitespace-delimited content\n    between delimiters as a list of separate values.\n\n    Use the ``ignore_expr`` argument to define expressions that may\n    contain opening or closing characters that should not be treated as\n    opening or closing characters for nesting, such as quoted_string or\n    a comment expression.  Specify multiple expressions using an\n    :class:`Or` or :class:`MatchFirst`. The default is\n    :class:`quoted_string`, but if no expressions are to be ignored, then\n    pass ``None`` for this argument.\n\n    Example::\n\n        data_type = one_of(\"void int short long char float double\")\n        decl_data_type = Combine(data_type + Opt(Word('*')))\n        ident = Word(alphas+'_', alphanums+'_')\n        number = pyparsing_common.number\n        arg = Group(decl_data_type + ident)\n        LPAR, RPAR = map(Suppress, \"()\")\n\n        code_body = nested_expr('{', '}', ignore_expr=(quoted_string | c_style_comment))\n\n        c_function = (decl_data_type(\"type\")\n                      + ident(\"name\")\n                      + LPAR + Opt(DelimitedList(arg), [])(\"args\") + RPAR\n                      + code_body(\"body\"))\n        c_function.ignore(c_style_comment)\n\n        source_code = '''\n            int is_odd(int x) {\n                return (x%2);\n            }\n\n            int dec_to_hex(char hchar) {\n                if (hchar >= '0' && hchar <= '9') {\n                    return (ord(hchar)-ord('0'));\n                } else {\n                    return (10+ord(hchar)-ord('A'));\n                }\n            }\n        '''\n        for func in c_function.search_string(source_code):\n            print(\"%(name)s (%(type)s) args: %(args)s\" % func)\n\n\n    prints::\n\n        is_odd (int) args: [['int', 'x']]\n        dec_to_hex (int) args: [['char', 'hchar']]\n    \"\"\"\n    if ignoreExpr != ignore_expr:\n        ignoreExpr = ignore_expr if ignoreExpr == quoted_string() else ignoreExpr\n    if opener == closer:\n        raise ValueError(\"opening and closing strings cannot be the same\")\n    if content is None:\n        if isinstance(opener, str_type) and isinstance(closer, str_type):\n            opener = typing.cast(str, opener)\n            closer = typing.cast(str, closer)\n            if len(opener) == 1 and len(closer) == 1:\n                if ignoreExpr is not None:\n                    content = Combine(\n                        OneOrMore(\n                            ~ignoreExpr\n                            + CharsNotIn(\n                                opener + closer + ParserElement.DEFAULT_WHITE_CHARS,\n                                exact=1,\n                            )\n                        )\n                    ).set_parse_action(lambda t: t[0].strip())\n                else:\n                    content = empty.copy() + CharsNotIn(\n                        opener + closer + ParserElement.DEFAULT_WHITE_CHARS\n                    ).set_parse_action(lambda t: t[0].strip())\n            else:\n                if ignoreExpr is not None:\n                    content = Combine(\n                        OneOrMore(\n                            ~ignoreExpr\n                            + ~Literal(opener)\n                            + ~Literal(closer)\n                            + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)\n                        )\n                    ).set_parse_action(lambda t: t[0].strip())\n                else:\n                    content = Combine(\n                        OneOrMore(\n                            ~Literal(opener)\n                            + ~Literal(closer)\n                            + CharsNotIn(ParserElement.DEFAULT_WHITE_CHARS, exact=1)\n                        )\n                    ).set_parse_action(lambda t: t[0].strip())\n        else:\n            raise ValueError(\n                \"opening and closing arguments must be strings if no content expression is given\"\n            )\n    ret = Forward()\n    if ignoreExpr is not None:\n        ret <<= Group(\n            Suppress(opener) + ZeroOrMore(ignoreExpr | ret | content) + Suppress(closer)\n        )\n    else:\n        ret <<= Group(Suppress(opener) + ZeroOrMore(ret | content) + Suppress(closer))\n    ret.set_name(\"nested %s%s expression\" % (opener, closer))\n    return ret\n\n\ndef _makeTags(tagStr, xml, suppress_LT=Suppress(\"<\"), suppress_GT=Suppress(\">\")):\n    \"\"\"Internal helper to construct opening and closing tag expressions, given a tag name\"\"\"\n    if isinstance(tagStr, str_type):\n        resname = tagStr\n        tagStr = Keyword(tagStr, caseless=not xml)\n    else:\n        resname = tagStr.name\n\n    tagAttrName = Word(alphas, alphanums + \"_-:\")\n    if xml:\n        tagAttrValue = dbl_quoted_string.copy().set_parse_action(remove_quotes)\n        openTag = (\n            suppress_LT\n            + tagStr(\"tag\")\n            + Dict(ZeroOrMore(Group(tagAttrName + Suppress(\"=\") + tagAttrValue)))\n            + Opt(\"/\", default=[False])(\"empty\").set_parse_action(\n                lambda s, l, t: t[0] == \"/\"\n            )\n            + suppress_GT\n        )\n    else:\n        tagAttrValue = quoted_string.copy().set_parse_action(remove_quotes) | Word(\n            printables, exclude_chars=\">\"\n        )\n        openTag = (\n            suppress_LT\n            + tagStr(\"tag\")\n            + Dict(\n                ZeroOrMore(\n                    Group(\n                        tagAttrName.set_parse_action(lambda t: t[0].lower())\n                        + Opt(Suppress(\"=\") + tagAttrValue)\n                    )\n                )\n            )\n            + Opt(\"/\", default=[False])(\"empty\").set_parse_action(\n                lambda s, l, t: t[0] == \"/\"\n            )\n            + suppress_GT\n        )\n    closeTag = Combine(Literal(\"</\") + tagStr + \">\", adjacent=False)\n\n    openTag.set_name(\"<%s>\" % resname)\n    # add start<tagname> results name in parse action now that ungrouped names are not reported at two levels\n    openTag.add_parse_action(\n        lambda t: t.__setitem__(\n            \"start\" + \"\".join(resname.replace(\":\", \" \").title().split()), t.copy()\n        )\n    )\n    closeTag = closeTag(\n        \"end\" + \"\".join(resname.replace(\":\", \" \").title().split())\n    ).set_name(\"</%s>\" % resname)\n    openTag.tag = resname\n    closeTag.tag = resname\n    openTag.tag_body = SkipTo(closeTag())\n    return openTag, closeTag\n\n\ndef make_html_tags(\n    tag_str: Union[str, ParserElement]\n) -> Tuple[ParserElement, ParserElement]:\n    \"\"\"Helper to construct opening and closing tag expressions for HTML,\n    given a tag name. Matches tags in either upper or lower case,\n    attributes with namespaces and with quoted or unquoted values.\n\n    Example::\n\n        text = '<td>More info at the <a href=\"https://github.com/pyparsing/pyparsing/wiki\">pyparsing</a> wiki page</td>'\n        # make_html_tags returns pyparsing expressions for the opening and\n        # closing tags as a 2-tuple\n        a, a_end = make_html_tags(\"A\")\n        link_expr = a + SkipTo(a_end)(\"link_text\") + a_end\n\n        for link in link_expr.search_string(text):\n            # attributes in the <A> tag (like \"href\" shown here) are\n            # also accessible as named results\n            print(link.link_text, '->', link.href)\n\n    prints::\n\n        pyparsing -> https://github.com/pyparsing/pyparsing/wiki\n    \"\"\"\n    return _makeTags(tag_str, False)\n\n\ndef make_xml_tags(\n    tag_str: Union[str, ParserElement]\n) -> Tuple[ParserElement, ParserElement]:\n    \"\"\"Helper to construct opening and closing tag expressions for XML,\n    given a tag name. Matches tags only in the given upper/lower case.\n\n    Example: similar to :class:`make_html_tags`\n    \"\"\"\n    return _makeTags(tag_str, True)\n\n\nany_open_tag: ParserElement\nany_close_tag: ParserElement\nany_open_tag, any_close_tag = make_html_tags(\n    Word(alphas, alphanums + \"_:\").set_name(\"any tag\")\n)\n\n_htmlEntityMap = {k.rstrip(\";\"): v for k, v in html.entities.html5.items()}\ncommon_html_entity = Regex(\"&(?P<entity>\" + \"|\".join(_htmlEntityMap) + \");\").set_name(\n    \"common HTML entity\"\n)\n\n\ndef replace_html_entity(s, l, t):\n    \"\"\"Helper parser action to replace common HTML entities with their special characters\"\"\"\n    return _htmlEntityMap.get(t.entity)\n\n\nclass OpAssoc(Enum):\n    \"\"\"Enumeration of operator associativity\n    - used in constructing InfixNotationOperatorSpec for :class:`infix_notation`\"\"\"\n\n    LEFT = 1\n    RIGHT = 2\n\n\nInfixNotationOperatorArgType = Union[\n    ParserElement, str, Tuple[Union[ParserElement, str], Union[ParserElement, str]]\n]\nInfixNotationOperatorSpec = Union[\n    Tuple[\n        InfixNotationOperatorArgType,\n        int,\n        OpAssoc,\n        typing.Optional[ParseAction],\n    ],\n    Tuple[\n        InfixNotationOperatorArgType,\n        int,\n        OpAssoc,\n    ],\n]\n\n\ndef infix_notation(\n    base_expr: ParserElement,\n    op_list: List[InfixNotationOperatorSpec],\n    lpar: Union[str, ParserElement] = Suppress(\"(\"),\n    rpar: Union[str, ParserElement] = Suppress(\")\"),\n) -> ParserElement:\n    \"\"\"Helper method for constructing grammars of expressions made up of\n    operators working in a precedence hierarchy.  Operators may be unary\n    or binary, left- or right-associative.  Parse actions can also be\n    attached to operator expressions. The generated parser will also\n    recognize the use of parentheses to override operator precedences\n    (see example below).\n\n    Note: if you define a deep operator list, you may see performance\n    issues when using infix_notation. See\n    :class:`ParserElement.enable_packrat` for a mechanism to potentially\n    improve your parser performance.\n\n    Parameters:\n\n    - ``base_expr`` - expression representing the most basic operand to\n      be used in the expression\n    - ``op_list`` - list of tuples, one for each operator precedence level\n      in the expression grammar; each tuple is of the form ``(op_expr,\n      num_operands, right_left_assoc, (optional)parse_action)``, where:\n\n      - ``op_expr`` is the pyparsing expression for the operator; may also\n        be a string, which will be converted to a Literal; if ``num_operands``\n        is 3, ``op_expr`` is a tuple of two expressions, for the two\n        operators separating the 3 terms\n      - ``num_operands`` is the number of terms for this operator (must be 1,\n        2, or 3)\n      - ``right_left_assoc`` is the indicator whether the operator is right\n        or left associative, using the pyparsing-defined constants\n        ``OpAssoc.RIGHT`` and ``OpAssoc.LEFT``.\n      - ``parse_action`` is the parse action to be associated with\n        expressions matching this operator expression (the parse action\n        tuple member may be omitted); if the parse action is passed\n        a tuple or list of functions, this is equivalent to calling\n        ``set_parse_action(*fn)``\n        (:class:`ParserElement.set_parse_action`)\n    - ``lpar`` - expression for matching left-parentheses; if passed as a\n      str, then will be parsed as ``Suppress(lpar)``. If lpar is passed as\n      an expression (such as ``Literal('(')``), then it will be kept in\n      the parsed results, and grouped with them. (default= ``Suppress('(')``)\n    - ``rpar`` - expression for matching right-parentheses; if passed as a\n      str, then will be parsed as ``Suppress(rpar)``. If rpar is passed as\n      an expression (such as ``Literal(')')``), then it will be kept in\n      the parsed results, and grouped with them. (default= ``Suppress(')')``)\n\n    Example::\n\n        # simple example of four-function arithmetic with ints and\n        # variable names\n        integer = pyparsing_common.signed_integer\n        varname = pyparsing_common.identifier\n\n        arith_expr = infix_notation(integer | varname,\n            [\n            ('-', 1, OpAssoc.RIGHT),\n            (one_of('* /'), 2, OpAssoc.LEFT),\n            (one_of('+ -'), 2, OpAssoc.LEFT),\n            ])\n\n        arith_expr.run_tests('''\n            5+3*6\n            (5+3)*6\n            -2--11\n            ''', full_dump=False)\n\n    prints::\n\n        5+3*6\n        [[5, '+', [3, '*', 6]]]\n\n        (5+3)*6\n        [[[5, '+', 3], '*', 6]]\n\n        (5+x)*y\n        [[[5, '+', 'x'], '*', 'y']]\n\n        -2--11\n        [[['-', 2], '-', ['-', 11]]]\n    \"\"\"\n\n    # captive version of FollowedBy that does not do parse actions or capture results names\n    class _FB(FollowedBy):\n        def parseImpl(self, instring, loc, doActions=True):\n            self.expr.try_parse(instring, loc)\n            return loc, []\n\n    _FB.__name__ = \"FollowedBy>\"\n\n    ret = Forward()\n    if isinstance(lpar, str):\n        lpar = Suppress(lpar)\n    if isinstance(rpar, str):\n        rpar = Suppress(rpar)\n\n    # if lpar and rpar are not suppressed, wrap in group\n    if not (isinstance(rpar, Suppress) and isinstance(rpar, Suppress)):\n        lastExpr = base_expr | Group(lpar + ret + rpar)\n    else:\n        lastExpr = base_expr | (lpar + ret + rpar)\n\n    arity: int\n    rightLeftAssoc: opAssoc\n    pa: typing.Optional[ParseAction]\n    opExpr1: ParserElement\n    opExpr2: ParserElement\n    for i, operDef in enumerate(op_list):\n        opExpr, arity, rightLeftAssoc, pa = (operDef + (None,))[:4]  # type: ignore[assignment]\n        if isinstance(opExpr, str_type):\n            opExpr = ParserElement._literalStringClass(opExpr)\n        opExpr = typing.cast(ParserElement, opExpr)\n        if arity == 3:\n            if not isinstance(opExpr, (tuple, list)) or len(opExpr) != 2:\n                raise ValueError(\n                    \"if numterms=3, opExpr must be a tuple or list of two expressions\"\n                )\n            opExpr1, opExpr2 = opExpr\n            term_name = f\"{opExpr1}{opExpr2} term\"\n        else:\n            term_name = f\"{opExpr} term\"\n\n        if not 1 <= arity <= 3:\n            raise ValueError(\"operator must be unary (1), binary (2), or ternary (3)\")\n\n        if rightLeftAssoc not in (OpAssoc.LEFT, OpAssoc.RIGHT):\n            raise ValueError(\"operator must indicate right or left associativity\")\n\n        thisExpr: ParserElement = Forward().set_name(term_name)\n        thisExpr = typing.cast(Forward, thisExpr)\n        if rightLeftAssoc is OpAssoc.LEFT:\n            if arity == 1:\n                matchExpr = _FB(lastExpr + opExpr) + Group(lastExpr + opExpr[1, ...])\n            elif arity == 2:\n                if opExpr is not None:\n                    matchExpr = _FB(lastExpr + opExpr + lastExpr) + Group(\n                        lastExpr + (opExpr + lastExpr)[1, ...]\n                    )\n                else:\n                    matchExpr = _FB(lastExpr + lastExpr) + Group(lastExpr[2, ...])\n            elif arity == 3:\n                matchExpr = _FB(\n                    lastExpr + opExpr1 + lastExpr + opExpr2 + lastExpr\n                ) + Group(lastExpr + OneOrMore(opExpr1 + lastExpr + opExpr2 + lastExpr))\n        elif rightLeftAssoc is OpAssoc.RIGHT:\n            if arity == 1:\n                # try to avoid LR with this extra test\n                if not isinstance(opExpr, Opt):\n                    opExpr = Opt(opExpr)\n                matchExpr = _FB(opExpr.expr + thisExpr) + Group(opExpr + thisExpr)\n            elif arity == 2:\n                if opExpr is not None:\n                    matchExpr = _FB(lastExpr + opExpr + thisExpr) + Group(\n                        lastExpr + (opExpr + thisExpr)[1, ...]\n                    )\n                else:\n                    matchExpr = _FB(lastExpr + thisExpr) + Group(\n                        lastExpr + thisExpr[1, ...]\n                    )\n            elif arity == 3:\n                matchExpr = _FB(\n                    lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr\n                ) + Group(lastExpr + opExpr1 + thisExpr + opExpr2 + thisExpr)\n        if pa:\n            if isinstance(pa, (tuple, list)):\n                matchExpr.set_parse_action(*pa)\n            else:\n                matchExpr.set_parse_action(pa)\n        thisExpr <<= (matchExpr | lastExpr).setName(term_name)\n        lastExpr = thisExpr\n    ret <<= lastExpr\n    return ret\n\n\ndef indentedBlock(blockStatementExpr, indentStack, indent=True, backup_stacks=[]):\n    \"\"\"\n    (DEPRECATED - use :class:`IndentedBlock` class instead)\n    Helper method for defining space-delimited indentation blocks,\n    such as those used to define block statements in Python source code.\n\n    Parameters:\n\n    - ``blockStatementExpr`` - expression defining syntax of statement that\n      is repeated within the indented block\n    - ``indentStack`` - list created by caller to manage indentation stack\n      (multiple ``statementWithIndentedBlock`` expressions within a single\n      grammar should share a common ``indentStack``)\n    - ``indent`` - boolean indicating whether block must be indented beyond\n      the current level; set to ``False`` for block of left-most statements\n      (default= ``True``)\n\n    A valid block must contain at least one ``blockStatement``.\n\n    (Note that indentedBlock uses internal parse actions which make it\n    incompatible with packrat parsing.)\n\n    Example::\n\n        data = '''\n        def A(z):\n          A1\n          B = 100\n          G = A2\n          A2\n          A3\n        B\n        def BB(a,b,c):\n          BB1\n          def BBA():\n            bba1\n            bba2\n            bba3\n        C\n        D\n        def spam(x,y):\n             def eggs(z):\n                 pass\n        '''\n\n\n        indentStack = [1]\n        stmt = Forward()\n\n        identifier = Word(alphas, alphanums)\n        funcDecl = (\"def\" + identifier + Group(\"(\" + Opt(delimitedList(identifier)) + \")\") + \":\")\n        func_body = indentedBlock(stmt, indentStack)\n        funcDef = Group(funcDecl + func_body)\n\n        rvalue = Forward()\n        funcCall = Group(identifier + \"(\" + Opt(delimitedList(rvalue)) + \")\")\n        rvalue << (funcCall | identifier | Word(nums))\n        assignment = Group(identifier + \"=\" + rvalue)\n        stmt << (funcDef | assignment | identifier)\n\n        module_body = stmt[1, ...]\n\n        parseTree = module_body.parseString(data)\n        parseTree.pprint()\n\n    prints::\n\n        [['def',\n          'A',\n          ['(', 'z', ')'],\n          ':',\n          [['A1'], [['B', '=', '100']], [['G', '=', 'A2']], ['A2'], ['A3']]],\n         'B',\n         ['def',\n          'BB',\n          ['(', 'a', 'b', 'c', ')'],\n          ':',\n          [['BB1'], [['def', 'BBA', ['(', ')'], ':', [['bba1'], ['bba2'], ['bba3']]]]]],\n         'C',\n         'D',\n         ['def',\n          'spam',\n          ['(', 'x', 'y', ')'],\n          ':',\n          [[['def', 'eggs', ['(', 'z', ')'], ':', [['pass']]]]]]]\n    \"\"\"\n    backup_stacks.append(indentStack[:])\n\n    def reset_stack():\n        indentStack[:] = backup_stacks[-1]\n\n    def checkPeerIndent(s, l, t):\n        if l >= len(s):\n            return\n        curCol = col(l, s)\n        if curCol != indentStack[-1]:\n            if curCol > indentStack[-1]:\n                raise ParseException(s, l, \"illegal nesting\")\n            raise ParseException(s, l, \"not a peer entry\")\n\n    def checkSubIndent(s, l, t):\n        curCol = col(l, s)\n        if curCol > indentStack[-1]:\n            indentStack.append(curCol)\n        else:\n            raise ParseException(s, l, \"not a subentry\")\n\n    def checkUnindent(s, l, t):\n        if l >= len(s):\n            return\n        curCol = col(l, s)\n        if not (indentStack and curCol in indentStack):\n            raise ParseException(s, l, \"not an unindent\")\n        if curCol < indentStack[-1]:\n            indentStack.pop()\n\n    NL = OneOrMore(LineEnd().set_whitespace_chars(\"\\t \").suppress())\n    INDENT = (Empty() + Empty().set_parse_action(checkSubIndent)).set_name(\"INDENT\")\n    PEER = Empty().set_parse_action(checkPeerIndent).set_name(\"\")\n    UNDENT = Empty().set_parse_action(checkUnindent).set_name(\"UNINDENT\")\n    if indent:\n        smExpr = Group(\n            Opt(NL)\n            + INDENT\n            + OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))\n            + UNDENT\n        )\n    else:\n        smExpr = Group(\n            Opt(NL)\n            + OneOrMore(PEER + Group(blockStatementExpr) + Opt(NL))\n            + Opt(UNDENT)\n        )\n\n    # add a parse action to remove backup_stack from list of backups\n    smExpr.add_parse_action(\n        lambda: backup_stacks.pop(-1) and None if backup_stacks else None\n    )\n    smExpr.set_fail_action(lambda a, b, c, d: reset_stack())\n    blockStatementExpr.ignore(_bslash + LineEnd())\n    return smExpr.set_name(\"indented block\")\n\n\n# it's easy to get these comment structures wrong - they're very common, so may as well make them available\nc_style_comment = Combine(Regex(r\"/\\*(?:[^*]|\\*(?!/))*\") + \"*/\").set_name(\n    \"C style comment\"\n)\n\"Comment of the form ``/* ... */``\"\n\nhtml_comment = Regex(r\"<!--[\\s\\S]*?-->\").set_name(\"HTML comment\")\n\"Comment of the form ``<!-- ... -->``\"\n\nrest_of_line = Regex(r\".*\").leave_whitespace().set_name(\"rest of line\")\ndbl_slash_comment = Regex(r\"//(?:\\\\\\n|[^\\n])*\").set_name(\"// comment\")\n\"Comment of the form ``// ... (to end of line)``\"\n\ncpp_style_comment = Combine(\n    Regex(r\"/\\*(?:[^*]|\\*(?!/))*\") + \"*/\" | dbl_slash_comment\n).set_name(\"C++ style comment\")\n\"Comment of either form :class:`c_style_comment` or :class:`dbl_slash_comment`\"\n\njava_style_comment = cpp_style_comment\n\"Same as :class:`cpp_style_comment`\"\n\npython_style_comment = Regex(r\"#.*\").set_name(\"Python style comment\")\n\"Comment of the form ``# ... (to end of line)``\"\n\n\n# build list of built-in expressions, for future reference if a global default value\n# gets updated\n_builtin_exprs: List[ParserElement] = [\n    v for v in vars().values() if isinstance(v, ParserElement)\n]\n\n\n# compatibility function, superseded by DelimitedList class\ndef delimited_list(\n    expr: Union[str, ParserElement],\n    delim: Union[str, ParserElement] = \",\",\n    combine: bool = False,\n    min: typing.Optional[int] = None,\n    max: typing.Optional[int] = None,\n    *,\n    allow_trailing_delim: bool = False,\n) -> ParserElement:\n    \"\"\"(DEPRECATED - use :class:`DelimitedList` class)\"\"\"\n    return DelimitedList(\n        expr, delim, combine, min, max, allow_trailing_delim=allow_trailing_delim\n    )\n\n\n# pre-PEP8 compatible names\n# fmt: off\nopAssoc = OpAssoc\nanyOpenTag = any_open_tag\nanyCloseTag = any_close_tag\ncommonHTMLEntity = common_html_entity\ncStyleComment = c_style_comment\nhtmlComment = html_comment\nrestOfLine = rest_of_line\ndblSlashComment = dbl_slash_comment\ncppStyleComment = cpp_style_comment\njavaStyleComment = java_style_comment\npythonStyleComment = python_style_comment\n\n@replaced_by_pep8(DelimitedList)\ndef delimitedList(): ...\n\n@replaced_by_pep8(DelimitedList)\ndef delimited_list(): ...\n\n@replaced_by_pep8(counted_array)\ndef countedArray(): ...\n\n@replaced_by_pep8(match_previous_literal)\ndef matchPreviousLiteral(): ...\n\n@replaced_by_pep8(match_previous_expr)\ndef matchPreviousExpr(): ...\n\n@replaced_by_pep8(one_of)\ndef oneOf(): ...\n\n@replaced_by_pep8(dict_of)\ndef dictOf(): ...\n\n@replaced_by_pep8(original_text_for)\ndef originalTextFor(): ...\n\n@replaced_by_pep8(nested_expr)\ndef nestedExpr(): ...\n\n@replaced_by_pep8(make_html_tags)\ndef makeHTMLTags(): ...\n\n@replaced_by_pep8(make_xml_tags)\ndef makeXMLTags(): ...\n\n@replaced_by_pep8(replace_html_entity)\ndef replaceHTMLEntity(): ...\n\n@replaced_by_pep8(infix_notation)\ndef infixNotation(): ...\n# fmt: on\n"},"hash":"60JDZqvEwO"}