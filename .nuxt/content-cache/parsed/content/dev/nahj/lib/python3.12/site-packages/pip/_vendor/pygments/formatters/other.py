{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:pip:_vendor:pygments:formatters:other.py","body":"\"\"\"\n    pygments.formatters.other\n    ~~~~~~~~~~~~~~~~~~~~~~~~~\n\n    Other formatters: NullFormatter, RawTokenFormatter.\n\n    :copyright: Copyright 2006-2023 by the Pygments team, see AUTHORS.\n    :license: BSD, see LICENSE for details.\n\"\"\"\n\nfrom pip._vendor.pygments.formatter import Formatter\nfrom pip._vendor.pygments.util import get_choice_opt\nfrom pip._vendor.pygments.token import Token\nfrom pip._vendor.pygments.console import colorize\n\n__all__ = ['NullFormatter', 'RawTokenFormatter', 'TestcaseFormatter']\n\n\nclass NullFormatter(Formatter):\n    \"\"\"\n    Output the text unchanged without any formatting.\n    \"\"\"\n    name = 'Text only'\n    aliases = ['text', 'null']\n    filenames = ['*.txt']\n\n    def format(self, tokensource, outfile):\n        enc = self.encoding\n        for ttype, value in tokensource:\n            if enc:\n                outfile.write(value.encode(enc))\n            else:\n                outfile.write(value)\n\n\nclass RawTokenFormatter(Formatter):\n    r\"\"\"\n    Format tokens as a raw representation for storing token streams.\n\n    The format is ``tokentype<TAB>repr(tokenstring)\\n``. The output can later\n    be converted to a token stream with the `RawTokenLexer`, described in the\n    :doc:`lexer list <lexers>`.\n\n    Only two options are accepted:\n\n    `compress`\n        If set to ``'gz'`` or ``'bz2'``, compress the output with the given\n        compression algorithm after encoding (default: ``''``).\n    `error_color`\n        If set to a color name, highlight error tokens using that color.  If\n        set but with no value, defaults to ``'red'``.\n\n        .. versionadded:: 0.11\n\n    \"\"\"\n    name = 'Raw tokens'\n    aliases = ['raw', 'tokens']\n    filenames = ['*.raw']\n\n    unicodeoutput = False\n\n    def __init__(self, **options):\n        Formatter.__init__(self, **options)\n        # We ignore self.encoding if it is set, since it gets set for lexer\n        # and formatter if given with -Oencoding on the command line.\n        # The RawTokenFormatter outputs only ASCII. Override here.\n        self.encoding = 'ascii'  # let pygments.format() do the right thing\n        self.compress = get_choice_opt(options, 'compress',\n                                       ['', 'none', 'gz', 'bz2'], '')\n        self.error_color = options.get('error_color', None)\n        if self.error_color is True:\n            self.error_color = 'red'\n        if self.error_color is not None:\n            try:\n                colorize(self.error_color, '')\n            except KeyError:\n                raise ValueError(\"Invalid color %r specified\" %\n                                 self.error_color)\n\n    def format(self, tokensource, outfile):\n        try:\n            outfile.write(b'')\n        except TypeError:\n            raise TypeError('The raw tokens formatter needs a binary '\n                            'output file')\n        if self.compress == 'gz':\n            import gzip\n            outfile = gzip.GzipFile('', 'wb', 9, outfile)\n\n            write = outfile.write\n            flush = outfile.close\n        elif self.compress == 'bz2':\n            import bz2\n            compressor = bz2.BZ2Compressor(9)\n\n            def write(text):\n                outfile.write(compressor.compress(text))\n\n            def flush():\n                outfile.write(compressor.flush())\n                outfile.flush()\n        else:\n            write = outfile.write\n            flush = outfile.flush\n\n        if self.error_color:\n            for ttype, value in tokensource:\n                line = b\"%r\\t%r\\n\" % (ttype, value)\n                if ttype is Token.Error:\n                    write(colorize(self.error_color, line))\n                else:\n                    write(line)\n        else:\n            for ttype, value in tokensource:\n                write(b\"%r\\t%r\\n\" % (ttype, value))\n        flush()\n\n\nTESTCASE_BEFORE = '''\\\n    def testNeedsName(lexer):\n        fragment = %r\n        tokens = [\n'''\nTESTCASE_AFTER = '''\\\n        ]\n        assert list(lexer.get_tokens(fragment)) == tokens\n'''\n\n\nclass TestcaseFormatter(Formatter):\n    \"\"\"\n    Format tokens as appropriate for a new testcase.\n\n    .. versionadded:: 2.0\n    \"\"\"\n    name = 'Testcase'\n    aliases = ['testcase']\n\n    def __init__(self, **options):\n        Formatter.__init__(self, **options)\n        if self.encoding is not None and self.encoding != 'utf-8':\n            raise ValueError(\"Only None and utf-8 are allowed encodings.\")\n\n    def format(self, tokensource, outfile):\n        indentation = ' ' * 12\n        rawbuf = []\n        outbuf = []\n        for ttype, value in tokensource:\n            rawbuf.append(value)\n            outbuf.append('%s(%s, %r),\\n' % (indentation, ttype, value))\n\n        before = TESTCASE_BEFORE % (''.join(rawbuf),)\n        during = ''.join(outbuf)\n        after = TESTCASE_AFTER\n        if self.encoding is None:\n            outfile.write(before + during + after)\n        else:\n            outfile.write(before.encode('utf-8'))\n            outfile.write(during.encode('utf-8'))\n            outfile.write(after.encode('utf-8'))\n        outfile.flush()\n"},"hash":"1hGnPyCNDh"}