{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:pip:_internal:index:collector.py","body":"\"\"\"\nThe main purpose of this module is to expose LinkCollector.collect_sources().\n\"\"\"\n\nimport collections\nimport email.message\nimport functools\nimport itertools\nimport json\nimport logging\nimport os\nimport urllib.parse\nimport urllib.request\nfrom html.parser import HTMLParser\nfrom optparse import Values\nfrom typing import (\n    TYPE_CHECKING,\n    Callable,\n    Dict,\n    Iterable,\n    List,\n    MutableMapping,\n    NamedTuple,\n    Optional,\n    Sequence,\n    Tuple,\n    Union,\n)\n\nfrom pip._vendor import requests\nfrom pip._vendor.requests import Response\nfrom pip._vendor.requests.exceptions import RetryError, SSLError\n\nfrom pip._internal.exceptions import NetworkConnectionError\nfrom pip._internal.models.link import Link\nfrom pip._internal.models.search_scope import SearchScope\nfrom pip._internal.network.session import PipSession\nfrom pip._internal.network.utils import raise_for_status\nfrom pip._internal.utils.filetypes import is_archive_file\nfrom pip._internal.utils.misc import redact_auth_from_url\nfrom pip._internal.vcs import vcs\n\nfrom .sources import CandidatesFromPage, LinkSource, build_source\n\nif TYPE_CHECKING:\n    from typing import Protocol\nelse:\n    Protocol = object\n\nlogger = logging.getLogger(__name__)\n\nResponseHeaders = MutableMapping[str, str]\n\n\ndef _match_vcs_scheme(url: str) -> Optional[str]:\n    \"\"\"Look for VCS schemes in the URL.\n\n    Returns the matched VCS scheme, or None if there's no match.\n    \"\"\"\n    for scheme in vcs.schemes:\n        if url.lower().startswith(scheme) and url[len(scheme)] in \"+:\":\n            return scheme\n    return None\n\n\nclass _NotAPIContent(Exception):\n    def __init__(self, content_type: str, request_desc: str) -> None:\n        super().__init__(content_type, request_desc)\n        self.content_type = content_type\n        self.request_desc = request_desc\n\n\ndef _ensure_api_header(response: Response) -> None:\n    \"\"\"\n    Check the Content-Type header to ensure the response contains a Simple\n    API Response.\n\n    Raises `_NotAPIContent` if the content type is not a valid content-type.\n    \"\"\"\n    content_type = response.headers.get(\"Content-Type\", \"Unknown\")\n\n    content_type_l = content_type.lower()\n    if content_type_l.startswith(\n        (\n            \"text/html\",\n            \"application/vnd.pypi.simple.v1+html\",\n            \"application/vnd.pypi.simple.v1+json\",\n        )\n    ):\n        return\n\n    raise _NotAPIContent(content_type, response.request.method)\n\n\nclass _NotHTTP(Exception):\n    pass\n\n\ndef _ensure_api_response(url: str, session: PipSession) -> None:\n    \"\"\"\n    Send a HEAD request to the URL, and ensure the response contains a simple\n    API Response.\n\n    Raises `_NotHTTP` if the URL is not available for a HEAD request, or\n    `_NotAPIContent` if the content type is not a valid content type.\n    \"\"\"\n    scheme, netloc, path, query, fragment = urllib.parse.urlsplit(url)\n    if scheme not in {\"http\", \"https\"}:\n        raise _NotHTTP()\n\n    resp = session.head(url, allow_redirects=True)\n    raise_for_status(resp)\n\n    _ensure_api_header(resp)\n\n\ndef _get_simple_response(url: str, session: PipSession) -> Response:\n    \"\"\"Access an Simple API response with GET, and return the response.\n\n    This consists of three parts:\n\n    1. If the URL looks suspiciously like an archive, send a HEAD first to\n       check the Content-Type is HTML or Simple API, to avoid downloading a\n       large file. Raise `_NotHTTP` if the content type cannot be determined, or\n       `_NotAPIContent` if it is not HTML or a Simple API.\n    2. Actually perform the request. Raise HTTP exceptions on network failures.\n    3. Check the Content-Type header to make sure we got a Simple API response,\n       and raise `_NotAPIContent` otherwise.\n    \"\"\"\n    if is_archive_file(Link(url).filename):\n        _ensure_api_response(url, session=session)\n\n    logger.debug(\"Getting page %s\", redact_auth_from_url(url))\n\n    resp = session.get(\n        url,\n        headers={\n            \"Accept\": \", \".join(\n                [\n                    \"application/vnd.pypi.simple.v1+json\",\n                    \"application/vnd.pypi.simple.v1+html; q=0.1\",\n                    \"text/html; q=0.01\",\n                ]\n            ),\n            # We don't want to blindly returned cached data for\n            # /simple/, because authors generally expecting that\n            # twine upload && pip install will function, but if\n            # they've done a pip install in the last ~10 minutes\n            # it won't. Thus by setting this to zero we will not\n            # blindly use any cached data, however the benefit of\n            # using max-age=0 instead of no-cache, is that we will\n            # still support conditional requests, so we will still\n            # minimize traffic sent in cases where the page hasn't\n            # changed at all, we will just always incur the round\n            # trip for the conditional GET now instead of only\n            # once per 10 minutes.\n            # For more information, please see pypa/pip#5670.\n            \"Cache-Control\": \"max-age=0\",\n        },\n    )\n    raise_for_status(resp)\n\n    # The check for archives above only works if the url ends with\n    # something that looks like an archive. However that is not a\n    # requirement of an url. Unless we issue a HEAD request on every\n    # url we cannot know ahead of time for sure if something is a\n    # Simple API response or not. However we can check after we've\n    # downloaded it.\n    _ensure_api_header(resp)\n\n    logger.debug(\n        \"Fetched page %s as %s\",\n        redact_auth_from_url(url),\n        resp.headers.get(\"Content-Type\", \"Unknown\"),\n    )\n\n    return resp\n\n\ndef _get_encoding_from_headers(headers: ResponseHeaders) -> Optional[str]:\n    \"\"\"Determine if we have any encoding information in our headers.\"\"\"\n    if headers and \"Content-Type\" in headers:\n        m = email.message.Message()\n        m[\"content-type\"] = headers[\"Content-Type\"]\n        charset = m.get_param(\"charset\")\n        if charset:\n            return str(charset)\n    return None\n\n\nclass CacheablePageContent:\n    def __init__(self, page: \"IndexContent\") -> None:\n        assert page.cache_link_parsing\n        self.page = page\n\n    def __eq__(self, other: object) -> bool:\n        return isinstance(other, type(self)) and self.page.url == other.page.url\n\n    def __hash__(self) -> int:\n        return hash(self.page.url)\n\n\nclass ParseLinks(Protocol):\n    def __call__(self, page: \"IndexContent\") -> Iterable[Link]:\n        ...\n\n\ndef with_cached_index_content(fn: ParseLinks) -> ParseLinks:\n    \"\"\"\n    Given a function that parses an Iterable[Link] from an IndexContent, cache the\n    function's result (keyed by CacheablePageContent), unless the IndexContent\n    `page` has `page.cache_link_parsing == False`.\n    \"\"\"\n\n    @functools.lru_cache(maxsize=None)\n    def wrapper(cacheable_page: CacheablePageContent) -> List[Link]:\n        return list(fn(cacheable_page.page))\n\n    @functools.wraps(fn)\n    def wrapper_wrapper(page: \"IndexContent\") -> List[Link]:\n        if page.cache_link_parsing:\n            return wrapper(CacheablePageContent(page))\n        return list(fn(page))\n\n    return wrapper_wrapper\n\n\n@with_cached_index_content\ndef parse_links(page: \"IndexContent\") -> Iterable[Link]:\n    \"\"\"\n    Parse a Simple API's Index Content, and yield its anchor elements as Link objects.\n    \"\"\"\n\n    content_type_l = page.content_type.lower()\n    if content_type_l.startswith(\"application/vnd.pypi.simple.v1+json\"):\n        data = json.loads(page.content)\n        for file in data.get(\"files\", []):\n            link = Link.from_json(file, page.url)\n            if link is None:\n                continue\n            yield link\n        return\n\n    parser = HTMLLinkParser(page.url)\n    encoding = page.encoding or \"utf-8\"\n    parser.feed(page.content.decode(encoding))\n\n    url = page.url\n    base_url = parser.base_url or url\n    for anchor in parser.anchors:\n        link = Link.from_element(anchor, page_url=url, base_url=base_url)\n        if link is None:\n            continue\n        yield link\n\n\nclass IndexContent:\n    \"\"\"Represents one response (or page), along with its URL\"\"\"\n\n    def __init__(\n        self,\n        content: bytes,\n        content_type: str,\n        encoding: Optional[str],\n        url: str,\n        cache_link_parsing: bool = True,\n    ) -> None:\n        \"\"\"\n        :param encoding: the encoding to decode the given content.\n        :param url: the URL from which the HTML was downloaded.\n        :param cache_link_parsing: whether links parsed from this page's url\n                                   should be cached. PyPI index urls should\n                                   have this set to False, for example.\n        \"\"\"\n        self.content = content\n        self.content_type = content_type\n        self.encoding = encoding\n        self.url = url\n        self.cache_link_parsing = cache_link_parsing\n\n    def __str__(self) -> str:\n        return redact_auth_from_url(self.url)\n\n\nclass HTMLLinkParser(HTMLParser):\n    \"\"\"\n    HTMLParser that keeps the first base HREF and a list of all anchor\n    elements' attributes.\n    \"\"\"\n\n    def __init__(self, url: str) -> None:\n        super().__init__(convert_charrefs=True)\n\n        self.url: str = url\n        self.base_url: Optional[str] = None\n        self.anchors: List[Dict[str, Optional[str]]] = []\n\n    def handle_starttag(self, tag: str, attrs: List[Tuple[str, Optional[str]]]) -> None:\n        if tag == \"base\" and self.base_url is None:\n            href = self.get_href(attrs)\n            if href is not None:\n                self.base_url = href\n        elif tag == \"a\":\n            self.anchors.append(dict(attrs))\n\n    def get_href(self, attrs: List[Tuple[str, Optional[str]]]) -> Optional[str]:\n        for name, value in attrs:\n            if name == \"href\":\n                return value\n        return None\n\n\ndef _handle_get_simple_fail(\n    link: Link,\n    reason: Union[str, Exception],\n    meth: Optional[Callable[..., None]] = None,\n) -> None:\n    if meth is None:\n        meth = logger.debug\n    meth(\"Could not fetch URL %s: %s - skipping\", link, reason)\n\n\ndef _make_index_content(\n    response: Response, cache_link_parsing: bool = True\n) -> IndexContent:\n    encoding = _get_encoding_from_headers(response.headers)\n    return IndexContent(\n        response.content,\n        response.headers[\"Content-Type\"],\n        encoding=encoding,\n        url=response.url,\n        cache_link_parsing=cache_link_parsing,\n    )\n\n\ndef _get_index_content(link: Link, *, session: PipSession) -> Optional[\"IndexContent\"]:\n    url = link.url.split(\"#\", 1)[0]\n\n    # Check for VCS schemes that do not support lookup as web pages.\n    vcs_scheme = _match_vcs_scheme(url)\n    if vcs_scheme:\n        logger.warning(\n            \"Cannot look at %s URL %s because it does not support lookup as web pages.\",\n            vcs_scheme,\n            link,\n        )\n        return None\n\n    # Tack index.html onto file:// URLs that point to directories\n    scheme, _, path, _, _, _ = urllib.parse.urlparse(url)\n    if scheme == \"file\" and os.path.isdir(urllib.request.url2pathname(path)):\n        # add trailing slash if not present so urljoin doesn't trim\n        # final segment\n        if not url.endswith(\"/\"):\n            url += \"/\"\n        # TODO: In the future, it would be nice if pip supported PEP 691\n        #       style responses in the file:// URLs, however there's no\n        #       standard file extension for application/vnd.pypi.simple.v1+json\n        #       so we'll need to come up with something on our own.\n        url = urllib.parse.urljoin(url, \"index.html\")\n        logger.debug(\" file: URL is directory, getting %s\", url)\n\n    try:\n        resp = _get_simple_response(url, session=session)\n    except _NotHTTP:\n        logger.warning(\n            \"Skipping page %s because it looks like an archive, and cannot \"\n            \"be checked by a HTTP HEAD request.\",\n            link,\n        )\n    except _NotAPIContent as exc:\n        logger.warning(\n            \"Skipping page %s because the %s request got Content-Type: %s. \"\n            \"The only supported Content-Types are application/vnd.pypi.simple.v1+json, \"\n            \"application/vnd.pypi.simple.v1+html, and text/html\",\n            link,\n            exc.request_desc,\n            exc.content_type,\n        )\n    except NetworkConnectionError as exc:\n        _handle_get_simple_fail(link, exc)\n    except RetryError as exc:\n        _handle_get_simple_fail(link, exc)\n    except SSLError as exc:\n        reason = \"There was a problem confirming the ssl certificate: \"\n        reason += str(exc)\n        _handle_get_simple_fail(link, reason, meth=logger.info)\n    except requests.ConnectionError as exc:\n        _handle_get_simple_fail(link, f\"connection error: {exc}\")\n    except requests.Timeout:\n        _handle_get_simple_fail(link, \"timed out\")\n    else:\n        return _make_index_content(resp, cache_link_parsing=link.cache_link_parsing)\n    return None\n\n\nclass CollectedSources(NamedTuple):\n    find_links: Sequence[Optional[LinkSource]]\n    index_urls: Sequence[Optional[LinkSource]]\n\n\nclass LinkCollector:\n\n    \"\"\"\n    Responsible for collecting Link objects from all configured locations,\n    making network requests as needed.\n\n    The class's main method is its collect_sources() method.\n    \"\"\"\n\n    def __init__(\n        self,\n        session: PipSession,\n        search_scope: SearchScope,\n    ) -> None:\n        self.search_scope = search_scope\n        self.session = session\n\n    @classmethod\n    def create(\n        cls,\n        session: PipSession,\n        options: Values,\n        suppress_no_index: bool = False,\n    ) -> \"LinkCollector\":\n        \"\"\"\n        :param session: The Session to use to make requests.\n        :param suppress_no_index: Whether to ignore the --no-index option\n            when constructing the SearchScope object.\n        \"\"\"\n        index_urls = [options.index_url] + options.extra_index_urls\n        if options.no_index and not suppress_no_index:\n            logger.debug(\n                \"Ignoring indexes: %s\",\n                \",\".join(redact_auth_from_url(url) for url in index_urls),\n            )\n            index_urls = []\n\n        # Make sure find_links is a list before passing to create().\n        find_links = options.find_links or []\n\n        search_scope = SearchScope.create(\n            find_links=find_links,\n            index_urls=index_urls,\n            no_index=options.no_index,\n        )\n        link_collector = LinkCollector(\n            session=session,\n            search_scope=search_scope,\n        )\n        return link_collector\n\n    @property\n    def find_links(self) -> List[str]:\n        return self.search_scope.find_links\n\n    def fetch_response(self, location: Link) -> Optional[IndexContent]:\n        \"\"\"\n        Fetch an HTML page containing package links.\n        \"\"\"\n        return _get_index_content(location, session=self.session)\n\n    def collect_sources(\n        self,\n        project_name: str,\n        candidates_from_page: CandidatesFromPage,\n    ) -> CollectedSources:\n        # The OrderedDict calls deduplicate sources by URL.\n        index_url_sources = collections.OrderedDict(\n            build_source(\n                loc,\n                candidates_from_page=candidates_from_page,\n                page_validator=self.session.is_secure_origin,\n                expand_dir=False,\n                cache_link_parsing=False,\n                project_name=project_name,\n            )\n            for loc in self.search_scope.get_index_urls_locations(project_name)\n        ).values()\n        find_links_sources = collections.OrderedDict(\n            build_source(\n                loc,\n                candidates_from_page=candidates_from_page,\n                page_validator=self.session.is_secure_origin,\n                expand_dir=True,\n                cache_link_parsing=True,\n                project_name=project_name,\n            )\n            for loc in self.find_links\n        ).values()\n\n        if logger.isEnabledFor(logging.DEBUG):\n            lines = [\n                f\"* {s.link}\"\n                for s in itertools.chain(find_links_sources, index_url_sources)\n                if s is not None and s.link is not None\n            ]\n            lines = [\n                f\"{len(lines)} location(s) to search \"\n                f\"for versions of {project_name}:\"\n            ] + lines\n            logger.debug(\"\\n\".join(lines))\n\n        return CollectedSources(\n            find_links=list(find_links_sources),\n            index_urls=list(index_url_sources),\n        )\n"},"hash":"a290F23M7y"}