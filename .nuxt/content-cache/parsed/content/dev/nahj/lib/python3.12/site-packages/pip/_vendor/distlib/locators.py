{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:pip:_vendor:distlib:locators.py","body":"# -*- coding: utf-8 -*-\n#\n# Copyright (C) 2012-2023 Vinay Sajip.\n# Licensed to the Python Software Foundation under a contributor agreement.\n# See LICENSE.txt and CONTRIBUTORS.txt.\n#\n\nimport gzip\nfrom io import BytesIO\nimport json\nimport logging\nimport os\nimport posixpath\nimport re\ntry:\n    import threading\nexcept ImportError:  # pragma: no cover\n    import dummy_threading as threading\nimport zlib\n\nfrom . import DistlibException\nfrom .compat import (urljoin, urlparse, urlunparse, url2pathname, pathname2url,\n                     queue, quote, unescape, build_opener,\n                     HTTPRedirectHandler as BaseRedirectHandler, text_type,\n                     Request, HTTPError, URLError)\nfrom .database import Distribution, DistributionPath, make_dist\nfrom .metadata import Metadata, MetadataInvalidError\nfrom .util import (cached_property, ensure_slash, split_filename, get_project_data,\n                   parse_requirement, parse_name_and_version, ServerProxy,\n                   normalize_name)\nfrom .version import get_scheme, UnsupportedVersionError\nfrom .wheel import Wheel, is_compatible\n\nlogger = logging.getLogger(__name__)\n\nHASHER_HASH = re.compile(r'^(\\w+)=([a-f0-9]+)')\nCHARSET = re.compile(r';\\s*charset\\s*=\\s*(.*)\\s*$', re.I)\nHTML_CONTENT_TYPE = re.compile('text/html|application/x(ht)?ml')\nDEFAULT_INDEX = 'https://pypi.org/pypi'\n\n\ndef get_all_distribution_names(url=None):\n    \"\"\"\n    Return all distribution names known by an index.\n    :param url: The URL of the index.\n    :return: A list of all known distribution names.\n    \"\"\"\n    if url is None:\n        url = DEFAULT_INDEX\n    client = ServerProxy(url, timeout=3.0)\n    try:\n        return client.list_packages()\n    finally:\n        client('close')()\n\n\nclass RedirectHandler(BaseRedirectHandler):\n    \"\"\"\n    A class to work around a bug in some Python 3.2.x releases.\n    \"\"\"\n    # There's a bug in the base version for some 3.2.x\n    # (e.g. 3.2.2 on Ubuntu Oneiric). If a Location header\n    # returns e.g. /abc, it bails because it says the scheme ''\n    # is bogus, when actually it should use the request's\n    # URL for the scheme. See Python issue #13696.\n    def http_error_302(self, req, fp, code, msg, headers):\n        # Some servers (incorrectly) return multiple Location headers\n        # (so probably same goes for URI).  Use first header.\n        newurl = None\n        for key in ('location', 'uri'):\n            if key in headers:\n                newurl = headers[key]\n                break\n        if newurl is None:  # pragma: no cover\n            return\n        urlparts = urlparse(newurl)\n        if urlparts.scheme == '':\n            newurl = urljoin(req.get_full_url(), newurl)\n            if hasattr(headers, 'replace_header'):\n                headers.replace_header(key, newurl)\n            else:\n                headers[key] = newurl\n        return BaseRedirectHandler.http_error_302(self, req, fp, code, msg,\n                                                  headers)\n\n    http_error_301 = http_error_303 = http_error_307 = http_error_302\n\n\nclass Locator(object):\n    \"\"\"\n    A base class for locators - things that locate distributions.\n    \"\"\"\n    source_extensions = ('.tar.gz', '.tar.bz2', '.tar', '.zip', '.tgz', '.tbz')\n    binary_extensions = ('.egg', '.exe', '.whl')\n    excluded_extensions = ('.pdf',)\n\n    # A list of tags indicating which wheels you want to match. The default\n    # value of None matches against the tags compatible with the running\n    # Python. If you want to match other values, set wheel_tags on a locator\n    # instance to a list of tuples (pyver, abi, arch) which you want to match.\n    wheel_tags = None\n\n    downloadable_extensions = source_extensions + ('.whl',)\n\n    def __init__(self, scheme='default'):\n        \"\"\"\n        Initialise an instance.\n        :param scheme: Because locators look for most recent versions, they\n                       need to know the version scheme to use. This specifies\n                       the current PEP-recommended scheme - use ``'legacy'``\n                       if you need to support existing distributions on PyPI.\n        \"\"\"\n        self._cache = {}\n        self.scheme = scheme\n        # Because of bugs in some of the handlers on some of the platforms,\n        # we use our own opener rather than just using urlopen.\n        self.opener = build_opener(RedirectHandler())\n        # If get_project() is called from locate(), the matcher instance\n        # is set from the requirement passed to locate(). See issue #18 for\n        # why this can be useful to know.\n        self.matcher = None\n        self.errors = queue.Queue()\n\n    def get_errors(self):\n        \"\"\"\n        Return any errors which have occurred.\n        \"\"\"\n        result = []\n        while not self.errors.empty():  # pragma: no cover\n            try:\n                e = self.errors.get(False)\n                result.append(e)\n            except self.errors.Empty:\n                continue\n            self.errors.task_done()\n        return result\n\n    def clear_errors(self):\n        \"\"\"\n        Clear any errors which may have been logged.\n        \"\"\"\n        # Just get the errors and throw them away\n        self.get_errors()\n\n    def clear_cache(self):\n        self._cache.clear()\n\n    def _get_scheme(self):\n        return self._scheme\n\n    def _set_scheme(self, value):\n        self._scheme = value\n\n    scheme = property(_get_scheme, _set_scheme)\n\n    def _get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This should be implemented in subclasses.\n\n        If called from a locate() request, self.matcher will be set to a\n        matcher for the requirement to satisfy, otherwise it will be None.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Please implement in the subclass')\n\n    def get_project(self, name):\n        \"\"\"\n        For a given project, get a dictionary mapping available versions to Distribution\n        instances.\n\n        This calls _get_project to do all the work, and just implements a caching layer on top.\n        \"\"\"\n        if self._cache is None:  # pragma: no cover\n            result = self._get_project(name)\n        elif name in self._cache:\n            result = self._cache[name]\n        else:\n            self.clear_errors()\n            result = self._get_project(name)\n            self._cache[name] = result\n        return result\n\n    def score_url(self, url):\n        \"\"\"\n        Give an url a score which can be used to choose preferred URLs\n        for a given project release.\n        \"\"\"\n        t = urlparse(url)\n        basename = posixpath.basename(t.path)\n        compatible = True\n        is_wheel = basename.endswith('.whl')\n        is_downloadable = basename.endswith(self.downloadable_extensions)\n        if is_wheel:\n            compatible = is_compatible(Wheel(basename), self.wheel_tags)\n        return (t.scheme == 'https', 'pypi.org' in t.netloc,\n                is_downloadable, is_wheel, compatible, basename)\n\n    def prefer_url(self, url1, url2):\n        \"\"\"\n        Choose one of two URLs where both are candidates for distribution\n        archives for the same version of a distribution (for example,\n        .tar.gz vs. zip).\n\n        The current implementation favours https:// URLs over http://, archives\n        from PyPI over those from other locations, wheel compatibility (if a\n        wheel) and then the archive name.\n        \"\"\"\n        result = url2\n        if url1:\n            s1 = self.score_url(url1)\n            s2 = self.score_url(url2)\n            if s1 > s2:\n                result = url1\n            if result != url2:\n                logger.debug('Not replacing %r with %r', url1, url2)\n            else:\n                logger.debug('Replacing %r with %r', url1, url2)\n        return result\n\n    def split_filename(self, filename, project_name):\n        \"\"\"\n        Attempt to split a filename in project name, version and Python version.\n        \"\"\"\n        return split_filename(filename, project_name)\n\n    def convert_url_to_download_info(self, url, project_name):\n        \"\"\"\n        See if a URL is a candidate for a download URL for a project (the URL\n        has typically been scraped from an HTML page).\n\n        If it is, a dictionary is returned with keys \"name\", \"version\",\n        \"filename\" and \"url\"; otherwise, None is returned.\n        \"\"\"\n        def same_project(name1, name2):\n            return normalize_name(name1) == normalize_name(name2)\n\n        result = None\n        scheme, netloc, path, params, query, frag = urlparse(url)\n        if frag.lower().startswith('egg='):  # pragma: no cover\n            logger.debug('%s: version hint in fragment: %r',\n                         project_name, frag)\n        m = HASHER_HASH.match(frag)\n        if m:\n            algo, digest = m.groups()\n        else:\n            algo, digest = None, None\n        origpath = path\n        if path and path[-1] == '/':  # pragma: no cover\n            path = path[:-1]\n        if path.endswith('.whl'):\n            try:\n                wheel = Wheel(path)\n                if not is_compatible(wheel, self.wheel_tags):\n                    logger.debug('Wheel not compatible: %s', path)\n                else:\n                    if project_name is None:\n                        include = True\n                    else:\n                        include = same_project(wheel.name, project_name)\n                    if include:\n                        result = {\n                            'name': wheel.name,\n                            'version': wheel.version,\n                            'filename': wheel.filename,\n                            'url': urlunparse((scheme, netloc, origpath,\n                                               params, query, '')),\n                            'python-version': ', '.join(\n                                ['.'.join(list(v[2:])) for v in wheel.pyver]),\n                        }\n            except Exception:  # pragma: no cover\n                logger.warning('invalid path for wheel: %s', path)\n        elif not path.endswith(self.downloadable_extensions):  # pragma: no cover\n            logger.debug('Not downloadable: %s', path)\n        else:  # downloadable extension\n            path = filename = posixpath.basename(path)\n            for ext in self.downloadable_extensions:\n                if path.endswith(ext):\n                    path = path[:-len(ext)]\n                    t = self.split_filename(path, project_name)\n                    if not t:  # pragma: no cover\n                        logger.debug('No match for project/version: %s', path)\n                    else:\n                        name, version, pyver = t\n                        if not project_name or same_project(project_name, name):\n                            result = {\n                                'name': name,\n                                'version': version,\n                                'filename': filename,\n                                'url': urlunparse((scheme, netloc, origpath,\n                                                   params, query, '')),\n                            }\n                            if pyver:  # pragma: no cover\n                                result['python-version'] = pyver\n                    break\n        if result and algo:\n            result['%s_digest' % algo] = digest\n        return result\n\n    def _get_digest(self, info):\n        \"\"\"\n        Get a digest from a dictionary by looking at a \"digests\" dictionary\n        or keys of the form 'algo_digest'.\n\n        Returns a 2-tuple (algo, digest) if found, else None. Currently\n        looks only for SHA256, then MD5.\n        \"\"\"\n        result = None\n        if 'digests' in info:\n            digests = info['digests']\n            for algo in ('sha256', 'md5'):\n                if algo in digests:\n                    result = (algo, digests[algo])\n                    break\n        if not result:\n            for algo in ('sha256', 'md5'):\n                key = '%s_digest' % algo\n                if key in info:\n                    result = (algo, info[key])\n                    break\n        return result\n\n    def _update_version_data(self, result, info):\n        \"\"\"\n        Update a result dictionary (the final result from _get_project) with a\n        dictionary for a specific version, which typically holds information\n        gleaned from a filename or URL for an archive for the distribution.\n        \"\"\"\n        name = info.pop('name')\n        version = info.pop('version')\n        if version in result:\n            dist = result[version]\n            md = dist.metadata\n        else:\n            dist = make_dist(name, version, scheme=self.scheme)\n            md = dist.metadata\n        dist.digest = digest = self._get_digest(info)\n        url = info['url']\n        result['digests'][url] = digest\n        if md.source_url != info['url']:\n            md.source_url = self.prefer_url(md.source_url, url)\n            result['urls'].setdefault(version, set()).add(url)\n        dist.locator = self\n        result[version] = dist\n\n    def locate(self, requirement, prereleases=False):\n        \"\"\"\n        Find the most recent distribution which matches the given\n        requirement.\n\n        :param requirement: A requirement of the form 'foo (1.0)' or perhaps\n                            'foo (>= 1.0, < 2.0, != 1.3)'\n        :param prereleases: If ``True``, allow pre-release versions\n                            to be located. Otherwise, pre-release versions\n                            are not returned.\n        :return: A :class:`Distribution` instance, or ``None`` if no such\n                 distribution could be located.\n        \"\"\"\n        result = None\n        r = parse_requirement(requirement)\n        if r is None:  # pragma: no cover\n            raise DistlibException('Not a valid requirement: %r' % requirement)\n        scheme = get_scheme(self.scheme)\n        self.matcher = matcher = scheme.matcher(r.requirement)\n        logger.debug('matcher: %s (%s)', matcher, type(matcher).__name__)\n        versions = self.get_project(r.name)\n        if len(versions) > 2:   # urls and digests keys are present\n            # sometimes, versions are invalid\n            slist = []\n            vcls = matcher.version_class\n            for k in versions:\n                if k in ('urls', 'digests'):\n                    continue\n                try:\n                    if not matcher.match(k):\n                        pass  # logger.debug('%s did not match %r', matcher, k)\n                    else:\n                        if prereleases or not vcls(k).is_prerelease:\n                            slist.append(k)\n                except Exception:  # pragma: no cover\n                    logger.warning('error matching %s with %r', matcher, k)\n                    pass  # slist.append(k)\n            if len(slist) > 1:\n                slist = sorted(slist, key=scheme.key)\n            if slist:\n                logger.debug('sorted list: %s', slist)\n                version = slist[-1]\n                result = versions[version]\n        if result:\n            if r.extras:\n                result.extras = r.extras\n            result.download_urls = versions.get('urls', {}).get(version, set())\n            d = {}\n            sd = versions.get('digests', {})\n            for url in result.download_urls:\n                if url in sd:  # pragma: no cover\n                    d[url] = sd[url]\n            result.digests = d\n        self.matcher = None\n        return result\n\n\nclass PyPIRPCLocator(Locator):\n    \"\"\"\n    This locator uses XML-RPC to locate distributions. It therefore\n    cannot be used with simple mirrors (that only mirror file content).\n    \"\"\"\n    def __init__(self, url, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param url: The URL to use for XML-RPC.\n        :param kwargs: Passed to the superclass constructor.\n        \"\"\"\n        super(PyPIRPCLocator, self).__init__(**kwargs)\n        self.base_url = url\n        self.client = ServerProxy(url, timeout=3.0)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        return set(self.client.list_packages())\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        versions = self.client.package_releases(name, True)\n        for v in versions:\n            urls = self.client.release_urls(name, v)\n            data = self.client.release_data(name, v)\n            metadata = Metadata(scheme=self.scheme)\n            metadata.name = data['name']\n            metadata.version = data['version']\n            metadata.license = data.get('license')\n            metadata.keywords = data.get('keywords', [])\n            metadata.summary = data.get('summary')\n            dist = Distribution(metadata)\n            if urls:\n                info = urls[0]\n                metadata.source_url = info['url']\n                dist.digest = self._get_digest(info)\n                dist.locator = self\n                result[v] = dist\n                for info in urls:\n                    url = info['url']\n                    digest = self._get_digest(info)\n                    result['urls'].setdefault(v, set()).add(url)\n                    result['digests'][url] = digest\n        return result\n\n\nclass PyPIJSONLocator(Locator):\n    \"\"\"\n    This locator uses PyPI's JSON interface. It's very limited in functionality\n    and probably not worth using.\n    \"\"\"\n    def __init__(self, url, **kwargs):\n        super(PyPIJSONLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        url = urljoin(self.base_url, '%s/json' % quote(name))\n        try:\n            resp = self.opener.open(url)\n            data = resp.read().decode()  # for now\n            d = json.loads(data)\n            md = Metadata(scheme=self.scheme)\n            data = d['info']\n            md.name = data['name']\n            md.version = data['version']\n            md.license = data.get('license')\n            md.keywords = data.get('keywords', [])\n            md.summary = data.get('summary')\n            dist = Distribution(md)\n            dist.locator = self\n            # urls = d['urls']\n            result[md.version] = dist\n            for info in d['urls']:\n                url = info['url']\n                dist.download_urls.add(url)\n                dist.digests[url] = self._get_digest(info)\n                result['urls'].setdefault(md.version, set()).add(url)\n                result['digests'][url] = self._get_digest(info)\n            # Now get other releases\n            for version, infos in d['releases'].items():\n                if version == md.version:\n                    continue    # already done\n                omd = Metadata(scheme=self.scheme)\n                omd.name = md.name\n                omd.version = version\n                odist = Distribution(omd)\n                odist.locator = self\n                result[version] = odist\n                for info in infos:\n                    url = info['url']\n                    odist.download_urls.add(url)\n                    odist.digests[url] = self._get_digest(info)\n                    result['urls'].setdefault(version, set()).add(url)\n                    result['digests'][url] = self._get_digest(info)\n#            for info in urls:\n#                md.source_url = info['url']\n#                dist.digest = self._get_digest(info)\n#                dist.locator = self\n#                for info in urls:\n#                    url = info['url']\n#                    result['urls'].setdefault(md.version, set()).add(url)\n#                    result['digests'][url] = self._get_digest(info)\n        except Exception as e:\n            self.errors.put(text_type(e))\n            logger.exception('JSON fetch failed: %s', e)\n        return result\n\n\nclass Page(object):\n    \"\"\"\n    This class represents a scraped HTML page.\n    \"\"\"\n    # The following slightly hairy-looking regex just looks for the contents of\n    # an anchor link, which has an attribute \"href\" either immediately preceded\n    # or immediately followed by a \"rel\" attribute. The attribute values can be\n    # declared with double quotes, single quotes or no quotes - which leads to\n    # the length of the expression.\n    _href = re.compile(\"\"\"\n(rel\\\\s*=\\\\s*(?:\"(?P<rel1>[^\"]*)\"|'(?P<rel2>[^']*)'|(?P<rel3>[^>\\\\s\\n]*))\\\\s+)?\nhref\\\\s*=\\\\s*(?:\"(?P<url1>[^\"]*)\"|'(?P<url2>[^']*)'|(?P<url3>[^>\\\\s\\n]*))\n(\\\\s+rel\\\\s*=\\\\s*(?:\"(?P<rel4>[^\"]*)\"|'(?P<rel5>[^']*)'|(?P<rel6>[^>\\\\s\\n]*)))?\n\"\"\", re.I | re.S | re.X)\n    _base = re.compile(r\"\"\"<base\\s+href\\s*=\\s*['\"]?([^'\">]+)\"\"\", re.I | re.S)\n\n    def __init__(self, data, url):\n        \"\"\"\n        Initialise an instance with the Unicode page contents and the URL they\n        came from.\n        \"\"\"\n        self.data = data\n        self.base_url = self.url = url\n        m = self._base.search(self.data)\n        if m:\n            self.base_url = m.group(1)\n\n    _clean_re = re.compile(r'[^a-z0-9$&+,/:;=?@.#%_\\\\|-]', re.I)\n\n    @cached_property\n    def links(self):\n        \"\"\"\n        Return the URLs of all the links on a page together with information\n        about their \"rel\" attribute, for determining which ones to treat as\n        downloads and which ones to queue for further scraping.\n        \"\"\"\n        def clean(url):\n            \"Tidy up an URL.\"\n            scheme, netloc, path, params, query, frag = urlparse(url)\n            return urlunparse((scheme, netloc, quote(path),\n                               params, query, frag))\n\n        result = set()\n        for match in self._href.finditer(self.data):\n            d = match.groupdict('')\n            rel = (d['rel1'] or d['rel2'] or d['rel3'] or\n                   d['rel4'] or d['rel5'] or d['rel6'])\n            url = d['url1'] or d['url2'] or d['url3']\n            url = urljoin(self.base_url, url)\n            url = unescape(url)\n            url = self._clean_re.sub(lambda m: '%%%2x' % ord(m.group(0)), url)\n            result.add((url, rel))\n        # We sort the result, hoping to bring the most recent versions\n        # to the front\n        result = sorted(result, key=lambda t: t[0], reverse=True)\n        return result\n\n\nclass SimpleScrapingLocator(Locator):\n    \"\"\"\n    A locator which scrapes HTML pages to locate downloads for a distribution.\n    This runs multiple threads to do the I/O; performance is at least as good\n    as pip's PackageFinder, which works in an analogous fashion.\n    \"\"\"\n\n    # These are used to deal with various Content-Encoding schemes.\n    decoders = {\n        'deflate': zlib.decompress,\n        'gzip': lambda b: gzip.GzipFile(fileobj=BytesIO(b)).read(),\n        'none': lambda b: b,\n    }\n\n    def __init__(self, url, timeout=None, num_workers=10, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param url: The root URL to use for scraping.\n        :param timeout: The timeout, in seconds, to be applied to requests.\n                        This defaults to ``None`` (no timeout specified).\n        :param num_workers: The number of worker threads you want to do I/O,\n                            This defaults to 10.\n        :param kwargs: Passed to the superclass.\n        \"\"\"\n        super(SimpleScrapingLocator, self).__init__(**kwargs)\n        self.base_url = ensure_slash(url)\n        self.timeout = timeout\n        self._page_cache = {}\n        self._seen = set()\n        self._to_fetch = queue.Queue()\n        self._bad_hosts = set()\n        self.skip_externals = False\n        self.num_workers = num_workers\n        self._lock = threading.RLock()\n        # See issue #45: we need to be resilient when the locator is used\n        # in a thread, e.g. with concurrent.futures. We can't use self._lock\n        # as it is for coordinating our internal threads - the ones created\n        # in _prepare_threads.\n        self._gplock = threading.RLock()\n        self.platform_check = False  # See issue #112\n\n    def _prepare_threads(self):\n        \"\"\"\n        Threads are created only when get_project is called, and terminate\n        before it returns. They are there primarily to parallelise I/O (i.e.\n        fetching web pages).\n        \"\"\"\n        self._threads = []\n        for i in range(self.num_workers):\n            t = threading.Thread(target=self._fetch)\n            t.daemon = True\n            t.start()\n            self._threads.append(t)\n\n    def _wait_threads(self):\n        \"\"\"\n        Tell all the threads to terminate (by sending a sentinel value) and\n        wait for them to do so.\n        \"\"\"\n        # Note that you need two loops, since you can't say which\n        # thread will get each sentinel\n        for t in self._threads:\n            self._to_fetch.put(None)    # sentinel\n        for t in self._threads:\n            t.join()\n        self._threads = []\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        with self._gplock:\n            self.result = result\n            self.project_name = name\n            url = urljoin(self.base_url, '%s/' % quote(name))\n            self._seen.clear()\n            self._page_cache.clear()\n            self._prepare_threads()\n            try:\n                logger.debug('Queueing %s', url)\n                self._to_fetch.put(url)\n                self._to_fetch.join()\n            finally:\n                self._wait_threads()\n            del self.result\n        return result\n\n    platform_dependent = re.compile(r'\\b(linux_(i\\d86|x86_64|arm\\w+)|'\n                                    r'win(32|_amd64)|macosx_?\\d+)\\b', re.I)\n\n    def _is_platform_dependent(self, url):\n        \"\"\"\n        Does an URL refer to a platform-specific download?\n        \"\"\"\n        return self.platform_dependent.search(url)\n\n    def _process_download(self, url):\n        \"\"\"\n        See if an URL is a suitable download for a project.\n\n        If it is, register information in the result dictionary (for\n        _get_project) about the specific version it's for.\n\n        Note that the return value isn't actually used other than as a boolean\n        value.\n        \"\"\"\n        if self.platform_check and self._is_platform_dependent(url):\n            info = None\n        else:\n            info = self.convert_url_to_download_info(url, self.project_name)\n        logger.debug('process_download: %s -> %s', url, info)\n        if info:\n            with self._lock:    # needed because self.result is shared\n                self._update_version_data(self.result, info)\n        return info\n\n    def _should_queue(self, link, referrer, rel):\n        \"\"\"\n        Determine whether a link URL from a referring page and with a\n        particular \"rel\" attribute should be queued for scraping.\n        \"\"\"\n        scheme, netloc, path, _, _, _ = urlparse(link)\n        if path.endswith(self.source_extensions + self.binary_extensions +\n                         self.excluded_extensions):\n            result = False\n        elif self.skip_externals and not link.startswith(self.base_url):\n            result = False\n        elif not referrer.startswith(self.base_url):\n            result = False\n        elif rel not in ('homepage', 'download'):\n            result = False\n        elif scheme not in ('http', 'https', 'ftp'):\n            result = False\n        elif self._is_platform_dependent(link):\n            result = False\n        else:\n            host = netloc.split(':', 1)[0]\n            if host.lower() == 'localhost':\n                result = False\n            else:\n                result = True\n        logger.debug('should_queue: %s (%s) from %s -> %s', link, rel,\n                     referrer, result)\n        return result\n\n    def _fetch(self):\n        \"\"\"\n        Get a URL to fetch from the work queue, get the HTML page, examine its\n        links for download candidates and candidates for further scraping.\n\n        This is a handy method to run in a thread.\n        \"\"\"\n        while True:\n            url = self._to_fetch.get()\n            try:\n                if url:\n                    page = self.get_page(url)\n                    if page is None:    # e.g. after an error\n                        continue\n                    for link, rel in page.links:\n                        if link not in self._seen:\n                            try:\n                                self._seen.add(link)\n                                if (not self._process_download(link) and\n                                        self._should_queue(link, url, rel)):\n                                    logger.debug('Queueing %s from %s', link, url)\n                                    self._to_fetch.put(link)\n                            except MetadataInvalidError:  # e.g. invalid versions\n                                pass\n            except Exception as e:  # pragma: no cover\n                self.errors.put(text_type(e))\n            finally:\n                # always do this, to avoid hangs :-)\n                self._to_fetch.task_done()\n            if not url:\n                # logger.debug('Sentinel seen, quitting.')\n                break\n\n    def get_page(self, url):\n        \"\"\"\n        Get the HTML for an URL, possibly from an in-memory cache.\n\n        XXX TODO Note: this cache is never actually cleared. It's assumed that\n        the data won't get stale over the lifetime of a locator instance (not\n        necessarily true for the default_locator).\n        \"\"\"\n        # http://peak.telecommunity.com/DevCenter/EasyInstall#package-index-api\n        scheme, netloc, path, _, _, _ = urlparse(url)\n        if scheme == 'file' and os.path.isdir(url2pathname(path)):\n            url = urljoin(ensure_slash(url), 'index.html')\n\n        if url in self._page_cache:\n            result = self._page_cache[url]\n            logger.debug('Returning %s from cache: %s', url, result)\n        else:\n            host = netloc.split(':', 1)[0]\n            result = None\n            if host in self._bad_hosts:\n                logger.debug('Skipping %s due to bad host %s', url, host)\n            else:\n                req = Request(url, headers={'Accept-encoding': 'identity'})\n                try:\n                    logger.debug('Fetching %s', url)\n                    resp = self.opener.open(req, timeout=self.timeout)\n                    logger.debug('Fetched %s', url)\n                    headers = resp.info()\n                    content_type = headers.get('Content-Type', '')\n                    if HTML_CONTENT_TYPE.match(content_type):\n                        final_url = resp.geturl()\n                        data = resp.read()\n                        encoding = headers.get('Content-Encoding')\n                        if encoding:\n                            decoder = self.decoders[encoding]   # fail if not found\n                            data = decoder(data)\n                        encoding = 'utf-8'\n                        m = CHARSET.search(content_type)\n                        if m:\n                            encoding = m.group(1)\n                        try:\n                            data = data.decode(encoding)\n                        except UnicodeError:  # pragma: no cover\n                            data = data.decode('latin-1')    # fallback\n                        result = Page(data, final_url)\n                        self._page_cache[final_url] = result\n                except HTTPError as e:\n                    if e.code != 404:\n                        logger.exception('Fetch failed: %s: %s', url, e)\n                except URLError as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                    with self._lock:\n                        self._bad_hosts.add(host)\n                except Exception as e:  # pragma: no cover\n                    logger.exception('Fetch failed: %s: %s', url, e)\n                finally:\n                    self._page_cache[url] = result   # even if None (failure)\n        return result\n\n    _distname_re = re.compile('<a href=[^>]*>([^<]+)<')\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        page = self.get_page(self.base_url)\n        if not page:\n            raise DistlibException('Unable to get %s' % self.base_url)\n        for match in self._distname_re.finditer(page.data):\n            result.add(match.group(1))\n        return result\n\n\nclass DirectoryLocator(Locator):\n    \"\"\"\n    This class locates distributions in a directory tree.\n    \"\"\"\n\n    def __init__(self, path, **kwargs):\n        \"\"\"\n        Initialise an instance.\n        :param path: The root of the directory tree to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * recursive - if True (the default), subdirectories are\n                         recursed into. If False, only the top-level directory\n                         is searched,\n        \"\"\"\n        self.recursive = kwargs.pop('recursive', True)\n        super(DirectoryLocator, self).__init__(**kwargs)\n        path = os.path.abspath(path)\n        if not os.path.isdir(path):  # pragma: no cover\n            raise DistlibException('Not a directory: %r' % path)\n        self.base_dir = path\n\n    def should_include(self, filename, parent):\n        \"\"\"\n        Should a filename be considered as a candidate for a distribution\n        archive? As well as the filename, the directory which contains it\n        is provided, though not used by the current implementation.\n        \"\"\"\n        return filename.endswith(self.downloadable_extensions)\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '',\n                                      pathname2url(os.path.abspath(fn)),\n                                      '', '', ''))\n                    info = self.convert_url_to_download_info(url, name)\n                    if info:\n                        self._update_version_data(result, info)\n            if not self.recursive:\n                break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for root, dirs, files in os.walk(self.base_dir):\n            for fn in files:\n                if self.should_include(fn, root):\n                    fn = os.path.join(root, fn)\n                    url = urlunparse(('file', '',\n                                      pathname2url(os.path.abspath(fn)),\n                                      '', '', ''))\n                    info = self.convert_url_to_download_info(url, None)\n                    if info:\n                        result.add(info['name'])\n            if not self.recursive:\n                break\n        return result\n\n\nclass JSONLocator(Locator):\n    \"\"\"\n    This locator uses special extended metadata (not available on PyPI) and is\n    the basis of performant dependency resolution in distlib. Other locators\n    require archive downloads before dependencies can be determined! As you\n    might imagine, that can be slow.\n    \"\"\"\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        raise NotImplementedError('Not available from this locator')\n\n    def _get_project(self, name):\n        result = {'urls': {}, 'digests': {}}\n        data = get_project_data(name)\n        if data:\n            for info in data.get('files', []):\n                if info['ptype'] != 'sdist' or info['pyversion'] != 'source':\n                    continue\n                # We don't store summary in project metadata as it makes\n                # the data bigger for no benefit during dependency\n                # resolution\n                dist = make_dist(data['name'], info['version'],\n                                 summary=data.get('summary',\n                                                  'Placeholder for summary'),\n                                 scheme=self.scheme)\n                md = dist.metadata\n                md.source_url = info['url']\n                # TODO SHA256 digest\n                if 'digest' in info and info['digest']:\n                    dist.digest = ('md5', info['digest'])\n                md.dependencies = info.get('requirements', {})\n                dist.exports = info.get('exports', {})\n                result[dist.version] = dist\n                result['urls'].setdefault(dist.version, set()).add(info['url'])\n        return result\n\n\nclass DistPathLocator(Locator):\n    \"\"\"\n    This locator finds installed distributions in a path. It can be useful for\n    adding to an :class:`AggregatingLocator`.\n    \"\"\"\n    def __init__(self, distpath, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param distpath: A :class:`DistributionPath` instance to search.\n        \"\"\"\n        super(DistPathLocator, self).__init__(**kwargs)\n        assert isinstance(distpath, DistributionPath)\n        self.distpath = distpath\n\n    def _get_project(self, name):\n        dist = self.distpath.get_distribution(name)\n        if dist is None:\n            result = {'urls': {}, 'digests': {}}\n        else:\n            result = {\n                dist.version: dist,\n                'urls': {dist.version: set([dist.source_url])},\n                'digests': {dist.version: set([None])}\n            }\n        return result\n\n\nclass AggregatingLocator(Locator):\n    \"\"\"\n    This class allows you to chain and/or merge a list of locators.\n    \"\"\"\n    def __init__(self, *locators, **kwargs):\n        \"\"\"\n        Initialise an instance.\n\n        :param locators: The list of locators to search.\n        :param kwargs: Passed to the superclass constructor,\n                       except for:\n                       * merge - if False (the default), the first successful\n                         search from any of the locators is returned. If True,\n                         the results from all locators are merged (this can be\n                         slow).\n        \"\"\"\n        self.merge = kwargs.pop('merge', False)\n        self.locators = locators\n        super(AggregatingLocator, self).__init__(**kwargs)\n\n    def clear_cache(self):\n        super(AggregatingLocator, self).clear_cache()\n        for locator in self.locators:\n            locator.clear_cache()\n\n    def _set_scheme(self, value):\n        self._scheme = value\n        for locator in self.locators:\n            locator.scheme = value\n\n    scheme = property(Locator.scheme.fget, _set_scheme)\n\n    def _get_project(self, name):\n        result = {}\n        for locator in self.locators:\n            d = locator.get_project(name)\n            if d:\n                if self.merge:\n                    files = result.get('urls', {})\n                    digests = result.get('digests', {})\n                    # next line could overwrite result['urls'], result['digests']\n                    result.update(d)\n                    df = result.get('urls')\n                    if files and df:\n                        for k, v in files.items():\n                            if k in df:\n                                df[k] |= v\n                            else:\n                                df[k] = v\n                    dd = result.get('digests')\n                    if digests and dd:\n                        dd.update(digests)\n                else:\n                    # See issue #18. If any dists are found and we're looking\n                    # for specific constraints, we only return something if\n                    # a match is found. For example, if a DirectoryLocator\n                    # returns just foo (1.0) while we're looking for\n                    # foo (>= 2.0), we'll pretend there was nothing there so\n                    # that subsequent locators can be queried. Otherwise we\n                    # would just return foo (1.0) which would then lead to a\n                    # failure to find foo (>= 2.0), because other locators\n                    # weren't searched. Note that this only matters when\n                    # merge=False.\n                    if self.matcher is None:\n                        found = True\n                    else:\n                        found = False\n                        for k in d:\n                            if self.matcher.match(k):\n                                found = True\n                                break\n                    if found:\n                        result = d\n                        break\n        return result\n\n    def get_distribution_names(self):\n        \"\"\"\n        Return all the distribution names known to this locator.\n        \"\"\"\n        result = set()\n        for locator in self.locators:\n            try:\n                result |= locator.get_distribution_names()\n            except NotImplementedError:\n                pass\n        return result\n\n\n# We use a legacy scheme simply because most of the dists on PyPI use legacy\n# versions which don't conform to PEP 440.\ndefault_locator = AggregatingLocator(\n                    # JSONLocator(), # don't use as PEP 426 is withdrawn\n                    SimpleScrapingLocator('https://pypi.org/simple/',\n                                          timeout=3.0),\n                    scheme='legacy')\n\nlocate = default_locator.locate\n\n\nclass DependencyFinder(object):\n    \"\"\"\n    Locate dependencies for distributions.\n    \"\"\"\n\n    def __init__(self, locator=None):\n        \"\"\"\n        Initialise an instance, using the specified locator\n        to locate distributions.\n        \"\"\"\n        self.locator = locator or default_locator\n        self.scheme = get_scheme(self.locator.scheme)\n\n    def add_distribution(self, dist):\n        \"\"\"\n        Add a distribution to the finder. This will update internal information\n        about who provides what.\n        :param dist: The distribution to add.\n        \"\"\"\n        logger.debug('adding distribution %s', dist)\n        name = dist.key\n        self.dists_by_name[name] = dist\n        self.dists[(name, dist.version)] = dist\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Add to provided: %s, %s, %s', name, version, dist)\n            self.provided.setdefault(name, set()).add((version, dist))\n\n    def remove_distribution(self, dist):\n        \"\"\"\n        Remove a distribution from the finder. This will update internal\n        information about who provides what.\n        :param dist: The distribution to remove.\n        \"\"\"\n        logger.debug('removing distribution %s', dist)\n        name = dist.key\n        del self.dists_by_name[name]\n        del self.dists[(name, dist.version)]\n        for p in dist.provides:\n            name, version = parse_name_and_version(p)\n            logger.debug('Remove from provided: %s, %s, %s', name, version, dist)\n            s = self.provided[name]\n            s.remove((version, dist))\n            if not s:\n                del self.provided[name]\n\n    def get_matcher(self, reqt):\n        \"\"\"\n        Get a version matcher for a requirement.\n        :param reqt: The requirement\n        :type reqt: str\n        :return: A version matcher (an instance of\n                 :class:`distlib.version.Matcher`).\n        \"\"\"\n        try:\n            matcher = self.scheme.matcher(reqt)\n        except UnsupportedVersionError:  # pragma: no cover\n            # XXX compat-mode if cannot read the version\n            name = reqt.split()[0]\n            matcher = self.scheme.matcher(name)\n        return matcher\n\n    def find_providers(self, reqt):\n        \"\"\"\n        Find the distributions which can fulfill a requirement.\n\n        :param reqt: The requirement.\n         :type reqt: str\n        :return: A set of distribution which can fulfill the requirement.\n        \"\"\"\n        matcher = self.get_matcher(reqt)\n        name = matcher.key   # case-insensitive\n        result = set()\n        provided = self.provided\n        if name in provided:\n            for version, provider in provided[name]:\n                try:\n                    match = matcher.match(version)\n                except UnsupportedVersionError:\n                    match = False\n\n                if match:\n                    result.add(provider)\n                    break\n        return result\n\n    def try_to_replace(self, provider, other, problems):\n        \"\"\"\n        Attempt to replace one provider with another. This is typically used\n        when resolving dependencies from multiple sources, e.g. A requires\n        (B >= 1.0) while C requires (B >= 1.1).\n\n        For successful replacement, ``provider`` must meet all the requirements\n        which ``other`` fulfills.\n\n        :param provider: The provider we are trying to replace with.\n        :param other: The provider we're trying to replace.\n        :param problems: If False is returned, this will contain what\n                         problems prevented replacement. This is currently\n                         a tuple of the literal string 'cantreplace',\n                         ``provider``, ``other``  and the set of requirements\n                         that ``provider`` couldn't fulfill.\n        :return: True if we can replace ``other`` with ``provider``, else\n                 False.\n        \"\"\"\n        rlist = self.reqts[other]\n        unmatched = set()\n        for s in rlist:\n            matcher = self.get_matcher(s)\n            if not matcher.match(provider.version):\n                unmatched.add(s)\n        if unmatched:\n            # can't replace other with provider\n            problems.add(('cantreplace', provider, other,\n                          frozenset(unmatched)))\n            result = False\n        else:\n            # can replace other with provider\n            self.remove_distribution(other)\n            del self.reqts[other]\n            for s in rlist:\n                self.reqts.setdefault(provider, set()).add(s)\n            self.add_distribution(provider)\n            result = True\n        return result\n\n    def find(self, requirement, meta_extras=None, prereleases=False):\n        \"\"\"\n        Find a distribution and all distributions it depends on.\n\n        :param requirement: The requirement specifying the distribution to\n                            find, or a Distribution instance.\n        :param meta_extras: A list of meta extras such as :test:, :build: and\n                            so on.\n        :param prereleases: If ``True``, allow pre-release versions to be\n                            returned - otherwise, don't return prereleases\n                            unless they're all that's available.\n\n        Return a set of :class:`Distribution` instances and a set of\n        problems.\n\n        The distributions returned should be such that they have the\n        :attr:`required` attribute set to ``True`` if they were\n        from the ``requirement`` passed to ``find()``, and they have the\n        :attr:`build_time_dependency` attribute set to ``True`` unless they\n        are post-installation dependencies of the ``requirement``.\n\n        The problems should be a tuple consisting of the string\n        ``'unsatisfied'`` and the requirement which couldn't be satisfied\n        by any distribution known to the locator.\n        \"\"\"\n\n        self.provided = {}\n        self.dists = {}\n        self.dists_by_name = {}\n        self.reqts = {}\n\n        meta_extras = set(meta_extras or [])\n        if ':*:' in meta_extras:\n            meta_extras.remove(':*:')\n            # :meta: and :run: are implicitly included\n            meta_extras |= set([':test:', ':build:', ':dev:'])\n\n        if isinstance(requirement, Distribution):\n            dist = odist = requirement\n            logger.debug('passed %s as requirement', odist)\n        else:\n            dist = odist = self.locator.locate(requirement,\n                                               prereleases=prereleases)\n            if dist is None:\n                raise DistlibException('Unable to locate %r' % requirement)\n            logger.debug('located %s', odist)\n        dist.requested = True\n        problems = set()\n        todo = set([dist])\n        install_dists = set([odist])\n        while todo:\n            dist = todo.pop()\n            name = dist.key     # case-insensitive\n            if name not in self.dists_by_name:\n                self.add_distribution(dist)\n            else:\n                # import pdb; pdb.set_trace()\n                other = self.dists_by_name[name]\n                if other != dist:\n                    self.try_to_replace(dist, other, problems)\n\n            ireqts = dist.run_requires | dist.meta_requires\n            sreqts = dist.build_requires\n            ereqts = set()\n            if meta_extras and dist in install_dists:\n                for key in ('test', 'build', 'dev'):\n                    e = ':%s:' % key\n                    if e in meta_extras:\n                        ereqts |= getattr(dist, '%s_requires' % key)\n            all_reqts = ireqts | sreqts | ereqts\n            for r in all_reqts:\n                providers = self.find_providers(r)\n                if not providers:\n                    logger.debug('No providers found for %r', r)\n                    provider = self.locator.locate(r, prereleases=prereleases)\n                    # If no provider is found and we didn't consider\n                    # prereleases, consider them now.\n                    if provider is None and not prereleases:\n                        provider = self.locator.locate(r, prereleases=True)\n                    if provider is None:\n                        logger.debug('Cannot satisfy %r', r)\n                        problems.add(('unsatisfied', r))\n                    else:\n                        n, v = provider.key, provider.version\n                        if (n, v) not in self.dists:\n                            todo.add(provider)\n                        providers.add(provider)\n                        if r in ireqts and dist in install_dists:\n                            install_dists.add(provider)\n                            logger.debug('Adding %s to install_dists',\n                                         provider.name_and_version)\n                for p in providers:\n                    name = p.key\n                    if name not in self.dists_by_name:\n                        self.reqts.setdefault(p, set()).add(r)\n                    else:\n                        other = self.dists_by_name[name]\n                        if other != p:\n                            # see if other can be replaced by p\n                            self.try_to_replace(p, other, problems)\n\n        dists = set(self.dists.values())\n        for dist in dists:\n            dist.build_time_dependency = dist not in install_dists\n            if dist.build_time_dependency:\n                logger.debug('%s is a build-time dependency only.',\n                             dist.name_and_version)\n        logger.debug('find done for %s', odist)\n        return dists, problems\n"},"hash":"clW5YmWVyw"}