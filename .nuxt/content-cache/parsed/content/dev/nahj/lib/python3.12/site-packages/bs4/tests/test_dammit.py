{"parsed":{"_id":"content:dev:nahj:lib:python3.12:site-packages:bs4:tests:test_dammit.py","body":"# encoding: utf-8\nimport pytest\nimport logging\nimport bs4\nfrom bs4 import BeautifulSoup\nfrom bs4.dammit import (\n    EntitySubstitution,\n    EncodingDetector,\n    UnicodeDammit,\n)\n\nclass TestUnicodeDammit(object):\n    \"\"\"Standalone tests of UnicodeDammit.\"\"\"\n\n    def test_unicode_input(self):\n        markup = \"I'm already Unicode! \\N{SNOWMAN}\"\n        dammit = UnicodeDammit(markup)\n        assert dammit.unicode_markup == markup\n\n    @pytest.mark.parametrize(\n        \"smart_quotes_to,expect_converted\",\n        [(None, \"\\u2018\\u2019\\u201c\\u201d\"),\n         (\"xml\", \"&#x2018;&#x2019;&#x201C;&#x201D;\"),\n         (\"html\", \"&lsquo;&rsquo;&ldquo;&rdquo;\"),\n         (\"ascii\", \"''\" + '\"\"'),\n        ]\n    )\n    def test_smart_quotes_to(self, smart_quotes_to, expect_converted):\n        \"\"\"Verify the functionality of the smart_quotes_to argument\n        to the UnicodeDammit constructor.\"\"\"\n        markup = b\"<foo>\\x91\\x92\\x93\\x94</foo>\"\n        converted = UnicodeDammit(\n            markup, known_definite_encodings=[\"windows-1252\"],\n            smart_quotes_to=smart_quotes_to\n        ).unicode_markup\n        assert converted == \"<foo>{}</foo>\".format(expect_converted)\n        \n    def test_detect_utf8(self):\n        utf8 = b\"Sacr\\xc3\\xa9 bleu! \\xe2\\x98\\x83\"\n        dammit = UnicodeDammit(utf8)\n        assert dammit.original_encoding.lower() == 'utf-8'\n        assert dammit.unicode_markup == 'Sacr\\xe9 bleu! \\N{SNOWMAN}'\n\n    def test_convert_hebrew(self):\n        hebrew = b\"\\xed\\xe5\\xec\\xf9\"\n        dammit = UnicodeDammit(hebrew, [\"iso-8859-8\"])\n        assert dammit.original_encoding.lower() == 'iso-8859-8'\n        assert dammit.unicode_markup == '\\u05dd\\u05d5\\u05dc\\u05e9'\n\n    def test_dont_see_smart_quotes_where_there_are_none(self):\n        utf_8 = b\"\\343\\202\\261\\343\\203\\274\\343\\202\\277\\343\\202\\244 Watch\"\n        dammit = UnicodeDammit(utf_8)\n        assert dammit.original_encoding.lower() == 'utf-8'\n        assert dammit.unicode_markup.encode(\"utf-8\") == utf_8\n\n    def test_ignore_inappropriate_codecs(self):\n        utf8_data = \"Räksmörgås\".encode(\"utf-8\")\n        dammit = UnicodeDammit(utf8_data, [\"iso-8859-8\"])\n        assert dammit.original_encoding.lower() == 'utf-8'\n\n    def test_ignore_invalid_codecs(self):\n        utf8_data = \"Räksmörgås\".encode(\"utf-8\")\n        for bad_encoding in ['.utf8', '...', 'utF---16.!']:\n            dammit = UnicodeDammit(utf8_data, [bad_encoding])\n            assert dammit.original_encoding.lower() == 'utf-8'\n\n    def test_exclude_encodings(self):\n        # This is UTF-8.\n        utf8_data = \"Räksmörgås\".encode(\"utf-8\")\n\n        # But if we exclude UTF-8 from consideration, the guess is\n        # Windows-1252.\n        dammit = UnicodeDammit(utf8_data, exclude_encodings=[\"utf-8\"])\n        assert dammit.original_encoding.lower() == 'windows-1252'\n\n        # And if we exclude that, there is no valid guess at all.\n        dammit = UnicodeDammit(\n            utf8_data, exclude_encodings=[\"utf-8\", \"windows-1252\"])\n        assert dammit.original_encoding == None\n\nclass TestEncodingDetector(object):\n        \n    def test_encoding_detector_replaces_junk_in_encoding_name_with_replacement_character(self):\n        detected = EncodingDetector(\n            b'<?xml version=\"1.0\" encoding=\"UTF-\\xdb\" ?>')\n        encodings = list(detected.encodings)\n        assert 'utf-\\N{REPLACEMENT CHARACTER}' in encodings\n\n    def test_detect_html5_style_meta_tag(self):\n\n        for data in (\n            b'<html><meta charset=\"euc-jp\" /></html>',\n            b\"<html><meta charset='euc-jp' /></html>\",\n            b\"<html><meta charset=euc-jp /></html>\",\n            b\"<html><meta charset=euc-jp/></html>\"):\n            dammit = UnicodeDammit(data, is_html=True)\n            assert \"euc-jp\" == dammit.original_encoding\n\n    def test_last_ditch_entity_replacement(self):\n        # This is a UTF-8 document that contains bytestrings\n        # completely incompatible with UTF-8 (ie. encoded with some other\n        # encoding).\n        #\n        # Since there is no consistent encoding for the document,\n        # Unicode, Dammit will eventually encode the document as UTF-8\n        # and encode the incompatible characters as REPLACEMENT\n        # CHARACTER.\n        #\n        # If chardet is installed, it will detect that the document\n        # can be converted into ISO-8859-1 without errors. This happens\n        # to be the wrong encoding, but it is a consistent encoding, so the\n        # code we're testing here won't run.\n        #\n        # So we temporarily disable chardet if it's present.\n        doc = b\"\"\"\\357\\273\\277<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n<html><b>\\330\\250\\330\\252\\330\\261</b>\n<i>\\310\\322\\321\\220\\312\\321\\355\\344</i></html>\"\"\"\n        chardet = bs4.dammit.chardet_dammit\n        logging.disable(logging.WARNING)\n        try:\n            def noop(str):\n                return None\n            bs4.dammit.chardet_dammit = noop\n            dammit = UnicodeDammit(doc)\n            assert True == dammit.contains_replacement_characters\n            assert \"\\ufffd\" in dammit.unicode_markup\n\n            soup = BeautifulSoup(doc, \"html.parser\")\n            assert soup.contains_replacement_characters\n        finally:\n            logging.disable(logging.NOTSET)\n            bs4.dammit.chardet_dammit = chardet\n\n    def test_byte_order_mark_removed(self):\n        # A document written in UTF-16LE will have its byte order marker stripped.\n        data = b'\\xff\\xfe<\\x00a\\x00>\\x00\\xe1\\x00\\xe9\\x00<\\x00/\\x00a\\x00>\\x00'\n        dammit = UnicodeDammit(data)\n        assert \"<a>áé</a>\" == dammit.unicode_markup\n        assert \"utf-16le\" == dammit.original_encoding\n       \n    def test_known_definite_versus_user_encodings(self):\n        # The known_definite_encodings are used before sniffing the\n        # byte-order mark; the user_encodings are used afterwards.\n\n        # Here's a document in UTF-16LE.\n        data = b'\\xff\\xfe<\\x00a\\x00>\\x00\\xe1\\x00\\xe9\\x00<\\x00/\\x00a\\x00>\\x00'\n        dammit = UnicodeDammit(data)\n\n        # We can process it as UTF-16 by passing it in as a known\n        # definite encoding.\n        before = UnicodeDammit(data, known_definite_encodings=[\"utf-16\"])\n        assert \"utf-16\" == before.original_encoding\n        \n        # If we pass UTF-18 as a user encoding, it's not even\n        # tried--the encoding sniffed from the byte-order mark takes\n        # precedence.\n        after = UnicodeDammit(data, user_encodings=[\"utf-8\"])\n        assert \"utf-16le\" == after.original_encoding\n        assert [\"utf-16le\"] == [x[0] for x in dammit.tried_encodings]\n        \n        # Here's a document in ISO-8859-8.\n        hebrew = b\"\\xed\\xe5\\xec\\xf9\"\n        dammit = UnicodeDammit(hebrew, known_definite_encodings=[\"utf-8\"],\n                               user_encodings=[\"iso-8859-8\"])\n        \n        # The known_definite_encodings don't work, BOM sniffing does\n        # nothing (it only works for a few UTF encodings), but one of\n        # the user_encodings does work.\n        assert \"iso-8859-8\" == dammit.original_encoding\n        assert [\"utf-8\", \"iso-8859-8\"] == [x[0] for x in dammit.tried_encodings]\n        \n    def test_deprecated_override_encodings(self):\n        # override_encodings is a deprecated alias for\n        # known_definite_encodings.\n        hebrew = b\"\\xed\\xe5\\xec\\xf9\"\n        dammit = UnicodeDammit(\n            hebrew,\n            known_definite_encodings=[\"shift-jis\"],\n            override_encodings=[\"utf-8\"],\n            user_encodings=[\"iso-8859-8\"],\n        )\n        assert \"iso-8859-8\" == dammit.original_encoding\n\n        # known_definite_encodings and override_encodings were tried\n        # before user_encodings.\n        assert [\"shift-jis\", \"utf-8\", \"iso-8859-8\"] == (\n            [x[0] for x in dammit.tried_encodings]\n        )\n\n    def test_detwingle(self):\n        # Here's a UTF8 document.\n        utf8 = (\"\\N{SNOWMAN}\" * 3).encode(\"utf8\")\n\n        # Here's a Windows-1252 document.\n        windows_1252 = (\n            \"\\N{LEFT DOUBLE QUOTATION MARK}Hi, I like Windows!\"\n            \"\\N{RIGHT DOUBLE QUOTATION MARK}\").encode(\"windows_1252\")\n\n        # Through some unholy alchemy, they've been stuck together.\n        doc = utf8 + windows_1252 + utf8\n\n        # The document can't be turned into UTF-8:\n        with pytest.raises(UnicodeDecodeError):\n            doc.decode(\"utf8\")\n\n        # Unicode, Dammit thinks the whole document is Windows-1252,\n        # and decodes it into \"â˜ƒâ˜ƒâ˜ƒ“Hi, I like Windows!”â˜ƒâ˜ƒâ˜ƒ\"\n\n        # But if we run it through fix_embedded_windows_1252, it's fixed:\n        fixed = UnicodeDammit.detwingle(doc)\n        assert \"☃☃☃“Hi, I like Windows!”☃☃☃\" == fixed.decode(\"utf8\")\n\n    def test_detwingle_ignores_multibyte_characters(self):\n        # Each of these characters has a UTF-8 representation ending\n        # in \\x93. \\x93 is a smart quote if interpreted as\n        # Windows-1252. But our code knows to skip over multibyte\n        # UTF-8 characters, so they'll survive the process unscathed.\n        for tricky_unicode_char in (\n            \"\\N{LATIN SMALL LIGATURE OE}\", # 2-byte char '\\xc5\\x93'\n            \"\\N{LATIN SUBSCRIPT SMALL LETTER X}\", # 3-byte char '\\xe2\\x82\\x93'\n            \"\\xf0\\x90\\x90\\x93\", # This is a CJK character, not sure which one.\n            ):\n            input = tricky_unicode_char.encode(\"utf8\")\n            assert input.endswith(b'\\x93')\n            output = UnicodeDammit.detwingle(input)\n            assert output == input\n\n    def test_find_declared_encoding(self):\n        # Test our ability to find a declared encoding inside an\n        # XML or HTML document.\n        #\n        # Even if the document comes in as Unicode, it may be\n        # interesting to know what encoding was claimed\n        # originally.\n\n        html_unicode = '<html><head><meta charset=\"utf-8\"></head></html>'\n        html_bytes = html_unicode.encode(\"ascii\")\n\n        xml_unicode= '<?xml version=\"1.0\" encoding=\"ISO-8859-1\" ?>'\n        xml_bytes = xml_unicode.encode(\"ascii\")\n\n        m = EncodingDetector.find_declared_encoding\n        assert m(html_unicode, is_html=False) is None\n        assert \"utf-8\" == m(html_unicode, is_html=True)\n        assert \"utf-8\" == m(html_bytes, is_html=True)\n\n        assert \"iso-8859-1\" == m(xml_unicode)\n        assert \"iso-8859-1\" == m(xml_bytes)\n\n        # Normally, only the first few kilobytes of a document are checked for\n        # an encoding.\n        spacer = b' ' * 5000\n        assert m(spacer + html_bytes) is None\n        assert m(spacer + xml_bytes) is None\n\n        # But you can tell find_declared_encoding to search an entire\n        # HTML document.\n        assert (\n            m(spacer + html_bytes, is_html=True, search_entire_document=True)\n            == \"utf-8\"\n        )\n\n        # The XML encoding declaration has to be the very first thing\n        # in the document. We'll allow whitespace before the document\n        # starts, but nothing else.\n        assert m(xml_bytes, search_entire_document=True) == \"iso-8859-1\"\n        assert m(b' ' + xml_bytes, search_entire_document=True) == \"iso-8859-1\"\n        assert m(b'a' + xml_bytes, search_entire_document=True) is None\n\n\nclass TestEntitySubstitution(object):\n    \"\"\"Standalone tests of the EntitySubstitution class.\"\"\"\n    def setup_method(self):\n        self.sub = EntitySubstitution\n\n\n    @pytest.mark.parametrize(\n        \"original,substituted\",\n        [\n            # Basic case. Unicode characters corresponding to named\n            # HTML entites are substituted; others are not.\n            (\"foo\\u2200\\N{SNOWMAN}\\u00f5bar\",\n             \"foo&forall;\\N{SNOWMAN}&otilde;bar\"),\n\n            # MS smart quotes are a common source of frustration, so we\n            # give them a special test.\n            ('‘’foo“”', \"&lsquo;&rsquo;foo&ldquo;&rdquo;\"),           \n        ]\n    )\n    def test_substitute_html(self, original, substituted):\n        assert self.sub.substitute_html(original) == substituted\n        \n    def test_html5_entity(self):\n        for entity, u in (\n            # A few spot checks of our ability to recognize\n            # special character sequences and convert them\n            # to named entities.\n            ('&models;', '\\u22a7'),\n            ('&Nfr;', '\\U0001d511'),\n            ('&ngeqq;', '\\u2267\\u0338'),\n            ('&not;', '\\xac'),\n            ('&Not;', '\\u2aec'),\n                \n            # We _could_ convert | to &verbarr;, but we don't, because\n            # | is an ASCII character.\n            ('|' '|'),\n\n            # Similarly for the fj ligature, which we could convert to\n            # &fjlig;, but we don't.\n            (\"fj\", \"fj\"),\n\n            # We do convert _these_ ASCII characters to HTML entities,\n            # because that's required to generate valid HTML.\n            ('&gt;', '>'),\n            ('&lt;', '<'),\n            ('&amp;', '&'),\n        ):\n            template = '3 %s 4'\n            raw = template % u\n            with_entities = template % entity\n            assert self.sub.substitute_html(raw) == with_entities\n            \n    def test_html5_entity_with_variation_selector(self):\n        # Some HTML5 entities correspond either to a single-character\n        # Unicode sequence _or_ to the same character plus U+FE00,\n        # VARIATION SELECTOR 1. We can handle this.\n        data = \"fjords \\u2294 penguins\"\n        markup = \"fjords &sqcup; penguins\"\n        assert self.sub.substitute_html(data) == markup\n\n        data = \"fjords \\u2294\\ufe00 penguins\"\n        markup = \"fjords &sqcups; penguins\"\n        assert self.sub.substitute_html(data) == markup\n        \n    def test_xml_converstion_includes_no_quotes_if_make_quoted_attribute_is_false(self):\n        s = 'Welcome to \"my bar\"'\n        assert self.sub.substitute_xml(s, False) == s\n\n    def test_xml_attribute_quoting_normally_uses_double_quotes(self):\n        assert self.sub.substitute_xml(\"Welcome\", True) == '\"Welcome\"'\n        assert self.sub.substitute_xml(\"Bob's Bar\", True) == '\"Bob\\'s Bar\"'\n\n    def test_xml_attribute_quoting_uses_single_quotes_when_value_contains_double_quotes(self):\n        s = 'Welcome to \"my bar\"'\n        assert self.sub.substitute_xml(s, True) == \"'Welcome to \\\"my bar\\\"'\"\n\n    def test_xml_attribute_quoting_escapes_single_quotes_when_value_contains_both_single_and_double_quotes(self):\n        s = 'Welcome to \"Bob\\'s Bar\"'\n        assert self.sub.substitute_xml(s, True) == '\"Welcome to &quot;Bob\\'s Bar&quot;\"'\n\n    def test_xml_quotes_arent_escaped_when_value_is_not_being_quoted(self):\n        quoted = 'Welcome to \"Bob\\'s Bar\"'\n        assert self.sub.substitute_xml(quoted) == quoted\n\n    def test_xml_quoting_handles_angle_brackets(self):\n        assert self.sub.substitute_xml(\"foo<bar>\") == \"foo&lt;bar&gt;\"\n\n    def test_xml_quoting_handles_ampersands(self):\n        assert self.sub.substitute_xml(\"AT&T\") == \"AT&amp;T\"\n\n    def test_xml_quoting_including_ampersands_when_they_are_part_of_an_entity(self):\n        assert self.sub.substitute_xml(\"&Aacute;T&T\") == \"&amp;Aacute;T&amp;T\"\n\n    def test_xml_quoting_ignoring_ampersands_when_they_are_part_of_an_entity(self):\n        assert self.sub.substitute_xml_containing_entities(\"&Aacute;T&T\") == \"&Aacute;T&amp;T\"\n       \n    def test_quotes_not_html_substituted(self):\n        \"\"\"There's no need to do this except inside attribute values.\"\"\"\n        text = 'Bob\\'s \"bar\"'\n        assert self.sub.substitute_html(text) == text\n"},"hash":"zGmGbr6576"}